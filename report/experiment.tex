This section describes the experimental setup of the thesis and aims to clarify the methods with which the hypothesis is tested.
As described in section \ref{sec:NN} and \ref{sec:SNN} there are at least two ways to approach the simulation of biological learning systems: ANNs and SNNs.
While the hypothesis is centred around SNN and their proximity to the behaviours expressed in the REF model, ANNs are architecturally proximate to SNNs and thus interesting from a conceptual viewpoint.
A vast amount of effort has been invested in the understanding of ANNs and their relation to the physiological domain \autocite{Nilsson2009, russel2007}.
Benchmarking the a SNN against an ANN allows for the comparison of traditional machine learning metrics like learning rate and accuracy between the two, regardless of the emerging properties of the REF model \autocite{schmidhuber2014, russel2007}.
Further, it lets the models relate more closely to the large body of research done in the field of artificial intelligence \autocite{Indiveri2015}.

The section is structured as follows.
First the nature and structure of the experiment is described and motivated.
Second, a practical approach to the modelling of neuronal networks is discussed.
Lastly the three backends on which the model will be evaluated are presented:
  a "regular" machine learning neural network, a neuronal network simulator and a neuromorphic hardware platform.

\section{Krechevsky maze experiment}
To test the hypothesis described in section \ref{hypothesis}, a maze experiment will be constructed.
Mazes have been employed in numerous animal experiment because of their small scale and limited scope \autocite{Krechevsky1932,Tolman1927}.
To encompass the diverse nature of the experiments a number of temporal and spatial arrangements have been devised \autocite{Tolman1927}.
A complicated maze is however outside of the scope of this thesis, since the maze features will have to be modelled in environments with limited capacity for complexity; particularly some neuromorphic hardware limits the degree of experimental complexity (see section \ref{sec:brainscales-limits}).
Further, it is desirable for the human experimenter to introspect the maze and the choices of the agents within the maze.
This would be practically impossible with a large and complex maze setup.
For these reasons the experiment is built around a simple Krechevsky T-maze \ref{Krechevsky1932}.
The maze is is sufficiently simply to be modelled in the experiment environments, while still retaining its relevance for the hypothesis.

A Krechevsky maze is dubbed after its inventor who experimented with rats in the early 20th century.
The maze is built as a T-maze with a series of consecutive left-right corridors.
Taking a wrong choice leads to a dead end, while the right choice leads to the next junction and eventually the exit (with a reward for the rats).
A significant property of the maze is that each junction is associated with one of more categorical features such as light/darkness, colour or shape.
One task could be for the rat to learn that the colour \texttt{red} consistently leads to a dead end, or that \texttt{darkness} consistently leads to the exit.
More sophisticated a maze could combine features (\texttt{red} and \texttt{darkness}) or let them alternate (first \texttt{red} then \texttt{darkness}) to lead the rat to its reward.

\section{Modelling learning systems} \label{sec:learning}
For the experiment, a model will be built in the learning model DSL Volr \autocite{Pedersen2018:volr-report}.

Volr is a declarative language designed to model \gls{NN}.
A network is constructed by describing a number of neurons (\texttt{population}) and the connections of these neurons to other \texttt{populations}.
Volr further supports detailing the type (inhibitory vs. excitatory), cardinality and weights for connections between \texttt{populations}, but is still missing a number of configuration options.
\footnote{TODO}

Volr offers a trade-off between complete, but verbose, descriptions of small networks and more general designs of large and complex networks.

 experiment the machine learning backend (described in section \ref{futhark}) and neuromorphic backend is
The benefit of Volr is that the same model can be evaluated on several

\subsection{Machine learning backend} \label{sec:futhark}
Evaluation of the machine learning

\subsection{Neural network simulation backend}

\subsection{Neuromorphic backend} \label{sec:neuromorphic}
