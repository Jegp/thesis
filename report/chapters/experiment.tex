\documentclass[report.tex]{subfiles}
\begin{document}
This chapter describes the experimental setup that aims to validate the
implementation and test the hypotheses introduced in section \ref{sec:hypotheses}.
Firstly, the assumptions and parameters that are the basis of the simulations are described.
Second, datasets and methods that are used to test the hypotheses are
elaborated.

\section{Neuron parameters}
\input{chapters/experiment/parameters}

\section{Parameter translation} \label{sec:translation}
\input{chapters/experiment/translation}

\section{Problem sets}
Three problem sets will be tested: the NAND ($\neg(A \land B)$) and XOR
($\oplus$) logical gates, as well as the 
Modified National Institute of Standards and Technology
(MNIST) database\index{MNIST}.
The NAND and XOR problems are trivial for \glspl{ANN} to learn, and are used as
a means to test and compare the rudimentary learning capacities of the NEST and
BrainScaleS backends.

The NAND and XOR experiments will be based on the same network topology
(\texttt{\textbf{dense} 2 4 $\obar$ \textbf{dense} 4 2}). 
All backends will execute the experiment with randomly initialised weights. However, the spiking backends will be evaluated
a second time with imported weights and biases from the optimised Futhark
networks.
This is interesting because Futhark is expected to outperform the \glspl{SNN}, and since the
network topology is shared, network parameters can be inserted 1:1.
In theory this should improve the initial training of the spiking models and
lead to an increased accuracy.

The weights and biases from the optimised Futhark model will only be imported into NEST,
which then trains the weights to fit the spiking neuron model.

The MNIST dataset is a widely used for training neural networks to classify
images of digits between 0 and 9. 
It is also commonly used for implementation benchmarks \cite{Schmidhuber2014,
Schmitt2017}, with the best networks scoring an error rate of 0.21\%
\cite{LeCun2019}.
MNIST consists of a collection of 60,000 training images and 10,000 testing images of handwritten digits \cite{LeCun1998}.

To predict the MNIST digits two networks will be constructed.
MNIST images contain 784 pixes (28x28), but to avoid too complex simulations
it is necessary to limit the network size.
The images have been cropped and scaled to 10x10 pixels, such that the initial
network layer can be scaled to 100 neurons.
The topology for the sequential model is \texttt{\textbf{dense} 100 100 $\obar$ \textbf{dense} 100 10}.

To test the parallel structures of the \gls{DSL}, a second and parallel network
will be constructed.
The network will resemble the sequential model, but consist of two separate
parallel subnetworks (\texttt{\textbf{dense} 20 10}), that is merged to produce
an output of 20 neurons.
The full model is as follows:
\texttt{\textbf{dense} 100 20 $\obar$ (\textbf{dense} 20 10 $\ominus$\
\textbf{dense} 20 10) $\obar$ \textbf{dense} 20 10}.
The idea of the model is that the two parallel subsystems can learn semantically
different tasks, and the final layer will be able to `choose' which subnetwork to
use, based on its weights.

\section{Experiment method}
All the above mentioned experiments are classification tasks, and the labels
are encoded as one-hot vectors.
To compare the network output with the labels, the argmax value of the network
output is taken and converted to a one-hot vector of the same shape as the label
data.

To avoid one-off effects such as local minima or (un)fortunate weight
initialisation, all experiments have been repeated 10 times.
The results reported below are accumulations of the prediction accuracies
and errors from the runs.

Weights have been initialised in the models using a normal distribution with a
mean of 1 and a standard deviation of 1.

The experiments use a 80/20 training/testing split with a fixed learning rate of
0.1, and the batch size has been set to 64.

To make the experiments as reproducible as possible, they have all been
initialised with constant random seeds.
Since all randomness in Futhark is based on this seed, all results are constant and
standard deviations are effectively 0.
This is not the case in PyNN, where the randomness is highly backend-specific.
A configuration for setting the initial seed exists
(\texttt{rng\_seed\_seeds})---and have been set for all experiments---but
PyNN does not fully support the randomness configurations in NEST
\cite{Gewaltig2007}.

\end{document}
