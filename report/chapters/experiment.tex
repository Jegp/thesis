\documentclass[report.tex]{subfiles}
\begin{document}
This chapter describes the experimental setup that aims to validate the
implementation and test the hypotheses introduced in section \ref{sec:hypotheses}.
Firstly, the assumptions and parameters that are the basis of the simulations are described. The second part elaborates on the datasets and methods that are used to test the hypotheses.

\section{Neuron parameters}
\input{chapters/experiment/parameters}

\section{Parameter translation}
\input{chapters/experiment/translation}

\section{Problem sets}
Three problem sets will be tested: the NAND ($\neg(A \land B)$) and XOR
($\oplus$) logical gates, as well as the 
Modified National Institute of Standards and Technology
(MNIST) database\index{MNIST}.
The NAND and XOR problems are trivial for \glspl{ANN} to learn, and are used as
a means to test and compare the rudimentary learning capacities of the NEST and
BrainScaleS backends.

The NAND and XOR experiments will be based on the same network topology
(\texttt{\textbf{dense} 2 4 $\obar$ \textbf{dense} 4 2}). 
All backends will execute the experiment with randomly initialised weights. However, the spiking backends will be evaluated
a second time with imported weights and biases from previously optimised networks, because the experiments are structurally similar, and Futhark is
expected to outperform the \glspl{SNN}.
The weights and biases from the optimised Futhark model will be imported into NEST,
and, after discretising them, the optimised weights and biases from NEST will be
inserted into the BrainScaleS model.
Because of the imprecision in the coding translation scheme between \glspl{ANN} and
\glspl{SNN}, this is expected to improve the performance of the \glspl{SNN} significantly.

%TODO KK check the following sentence. Looks weird, but maybe it'S fine?
The MNIST network will 
\texttt{\textbf{dense} (\textbf{dense} 2 2 $\ominus$ \textbf{dense} 2 2) $\obar$ \textbf{dense} 4 2}.
The two networks exist to compare the performance of the parallel structure with 
a traditional sequential one.

The MNIST dataset is a widely used for training neural networks to classify digits (0 - 9),and is commonly used for implementation benchmarks \cite{Schmidhuber2014, Schmitt2017}. It is a famous collection of 60,000 training images and 10,000 testing images of handwritten digits \cite{LeCun1998}.

To solve this problem set, a sequential and a parallel network will be constructed:
\texttt{\textbf{dense} 100 20 $\obar$ \textbf{dense} 20 10} and
\texttt{(\textbf{dense} 100 10 $\ominus$ \textbf{dense} 100 10) $\obar$ \textbf{dense 20 10}}.

\begin{comment}

\subsection{Neuromorphic backend} \label{sec:neuromorphic}
% \subsubsection{Experiment stimuli}
% The stimuli describes the ``input'' of the model.
% Such input is defined either as an array of elements directly in the DSL
% or as a reference to a file.

% \subsubsection{Experiment populations}
% The populations describe the topology of the neural network itself.
% As with the stimuli, the populations are built around a block structure that
% contains a number of sub-expressions.

% The \texttt{connection} defines the source stimulus for the population,
% i.e. the population \textit{from} which action-potentials will be forwarded.
% A population can receive stimulus from more than one source.
% The connections are modelled as per the \gls{CSA} described in section
% \ref{sec:volr-csa}.

% % TODO: Describe and invent archetypes... or not?

% \subsubsection{Experiment responses}
% The responses are the ``output'' of the model to be recorded, and can be
% considered as the outcome of the network for training purposes.
% The response block only contains an optional specification of a location for
% the experiment output data.

\end{comment}
\end{document}
