\documentclass[report.tex]{subfiles}
\begin{document}

This first chapter presents the theoretical foundation for the thesis.
The theory is grouped into three sections: \glspl{NN}, learning and
neuromorphic hardware.\index{neuromorphic}

\section{Neural networks} \label{sec:nn}
\Glspl{NN} is a broad term that originates in the neuronal models from
the biological brain \cite{Dayan2001}.
The general architecture of neural systems can be explained as circuits
of neurons \index{neuron} connected through weighted edges
\cite{Russel2007, Dayan2001}.

In this abstract sense, a neuron is a computational unit that
takes a number of signals (inputs) and processes them through some
function $f$,that outputs a single value \cite{Eliasmith2004}.
Composed in a network, neurons can \textit{compute} 
complex non-linear functions \cite{Eliasmith2004, Dayan2001}.

In a more concrete sense, \gls{NN}s compute over either
continuous (e.g., voltage and numbers) or discrete signals
\cite{Russel2007, Schmidhuber2014}.
Discrete models were the foundation for
the first generation of neural networks \cite{Russel2007, Maass1997}.
They are based on the perceptron model as seen in Equation
\ref{eq:perceptron}, also known as the McCulluch-Pitts neuron model
\cite{Eliasmith2004}.

\begin{equation} \label{eq:perceptron}
\sigma(x) = \begin{cases}
	 1 & \text{if } x > 0\\
	 0 & \text{otherwise}
       \end{cases}
\end{equation}

\subsection{Neural networks as directed graphs}
These first \glspl{NN} would collect neurons in groups\index{neuron!group} 
that connect to other groups in a sequence
\cite{Russel2007}.
Figure \ref{fig:nn-example} shows an example of such a network.
\begin{figure}
\centering
\tikzset{%
  every neuron/.style={
    circle, draw, minimum size=0.5cm
  },
  neuron dots/.style={
    draw=none,
    scale=1.5,
    text height=0.3cm,
    execute at begin node=\color{black}$\vdots$
  }
}
\begin{tikzpicture}{x=1.5cm, y=1.5cm}
  \foreach \m [count=\y] in {1,2,dots,n}
    \node [every neuron/.try, neuron \m/.try](input-\m) at (0, 2.5-\y) {};
  \foreach \m [count=\y] in {1, dots, n}
    \node [every neuron/.try, neuron \m/.try](hidden-\m) at (2, 2-\y) {};
  \foreach \m [count=\y] in {1, dots, n}
    \node [every neuron/.try, neuron \m/.try](output-\m) at (4, 2-\y) {};

  \foreach \l [count=\i] in {1, 2, n}
    \draw [<-] (input-\l) -- ++ (-1, 0)
      node [above, midway] {$I_{\l}$};
  \node [above] at (hidden-1.north) {$H_1$};
  \node [below] at (hidden-n.south) {$H_n$};

  \foreach \f in {1, 2, n}
    \foreach \t in {1, n}
      \draw [->] (input-\f) -- (hidden-\t);
  \foreach \f in {1, n}
    \foreach \t in {1, n}
      \draw [->] (hidden-\f) -- (output-\t);
  \foreach \l [count=\i] in {1,n}
    \draw [->] (output-\l) -- ++(1,0)
	node [above, midway] {$O_\l$};

  \foreach \l [count=\x from 0] in {Input, Hidden, Output}
    \node [align=center, above] at (\x*2,2) {\l};
  
  \draw [decorate,decoration={brace, amplitude=5pt,mirror,raise=4ex}]
    (0,-1.2) -- (1.8,-1.2) node[midway,yshift=-3em]{$l_1$};
  \draw [decorate,decoration={brace, amplitude=5pt,mirror,raise=4ex}]
    (2,-1.2) -- (3.8,-1.2) node[midway,yshift=-3em]{$l_2$};
\end{tikzpicture}
\caption{An example neural network of depth 3 with two layers ($l_1$, $l_2$) and a single hidden neuron group ($H$).}
\label{fig:nn-example}
\end{figure}

The number of groups determines the depth\index{neural network!depth} of
a network\cite{Russel2007}.
Each group applies a non-linear transformation to the input that is forwarded
to the next layer and so on \cite{Bishop2006, Russel2007}.
From a computational point of view a neuron group is simply a computational
unit, which allows \glspl{NN} to be abstracted as circuits of units connected in
a directed graph
\cite{Dayan2001, Eliasmith2004, Russel2007}.
This view can be simplified as shown in figure \ref{fig:nn-composition},
such that each neuron group\index{neuron!group} (node)\index{node}
is considered a function \cite{Rojas1996}.
Here the output is found by the sequential
composition of activation functions over the input $x$.
\begin{figure}
\centering
\tikzset{%
  node/.style={
    circle, draw, minimum size=0.8cm
  }
}
\begin{tikzpicture}
  \node (input) at (-0.3,0) { $x$ };
  \node [node] (node-f) at (1.5,0) { $g$ };
  \node [node] (node-g) at (3,0) { $h$ };
  \node (output) at (5,0) { $g(h(x))$ };
  \draw [->] (input) -- (node-f);
  \draw [->] (node-f) -- (node-g);
  \draw [->] (node-g) -- (output);
\end{tikzpicture}
\caption{Another representation of the network in Figure
  \ref{fig:nn-example}, where each layer is considered a function ($l_1 = g$, $l_2 = h$),
and the output is derived by composing functions sequentially 
over the input ($x$).}
\label{fig:nn-composition}
\end{figure}

Neuron groups are sometimes referred to as \textit{layers}\index{neural network!layer} \index{layer|see neural network}
in the literature, but from a computational perspective it is simpler to view layers as
functions, such that they include the output activations for the next neuron group \cite{Bishop2006}.
In this thesis, a layer is defined as computational units that transform input
with a non-linear function to produce some output.
Thus the network in figure \ref{fig:nn-example} consists of two layers.

The layered design is typical for first and second generation networks, but 
can be generalised to the third generation as well, despite their parallel
nature \cite{Eliasmith2015}. %TODO: Move to 3rd gen
Neuron groups or layers that are not directly connected to the input or output
of the network is traditionally denoted as `hidden'\index{layer!hidden} because it is only
stimulated indirectly \cite{Russel2007}.

In this representation the `input' is a vector, whose
length is equal to the number of input neurons in the first layer.
Conversely, the `output' is a vector whose length
is controlled by the number of output neurons in the network.
An \gls{NN} can then be understood as a function $f$ that 
maps an input vector to an output vector of arbitrary size
\cite{Russel2007}.

Neuron models typically enrich the
input signals ($x$) with a linear equation as shown in Equation \ref{eq:weightedoutput},
where $\sigma$ is the neuron function
\cite{Schmidhuber2014, Russel2007}. 
For a single neuron $x$, the output signal is calculated through a weight ($w$)
and a bias ($b$). 
Weights and biases allow the model to adapt the relative importance of each
input neuron by modifying their weights and biases, thus allowing the
model to \textit{train} to a given domain \cite{Schmidhuber2014, Russel2007}.

For each neuron $x$ in the layer $j$, the output $x_j$ can be calculated given the activation value,
weight and bias from the previous layer ($i$) (see Equation
\ref{eq:weightedoutput}).
Here $u_j$ is the weighted sum of the output from the previous layer $i$, before
applying the activation function $\sigma$.

\begin{equation} \label{eq:weightedoutput}
x_j = \sigma(u_j) = \sigma\left(\left[\sum_{i=1}^n w_{i,j} x_i\right] + b_j \right)
\end{equation}

\subsection{Second generation neural networks}
Second generation neural networks augment the perceptron model by
a) allowing continuous output values of a neuron and b) parametrising
the computation of the neuron by adding an \textit{activation function}
\index{activation function} that determines the output of the neuron
\cite{Maass1997}.
\textit{Sigmoidal} functions are commonly used for activation functions
because they resemble the perceptron step function while 
retaining continuity (see Figure \ref{fig:sigmoid})
\cite{Maass1997}.

\begin{figure}
\centering
\begin{tikzpicture}[domain=-6:6,xscale=0.3]
  \draw[->] (-6.2,0) -- (6.2,0) node[right] {$x$};
  \draw[->] (0,-0.2) -- (0,1.2) node[above] {$\sigma(x)$};
  \draw plot (\x,{1 / (1 + exp(-\x))}) node[right] {$\sigma(x) = {1 \over 1 + e^{-x}}$};
\end{tikzpicture}
\caption{A sigmoidal (soft step) function.}
\label{fig:sigmoid}
\end{figure}

A number of variations for sigmoidal activation functions exist such as the 
hyperbolic tangent ($tanh$) and
the rectified linear unit \index{ReLU} (ReLU, see Equation \ref{eq:ReLU}). 
They are applied either in a feed-forward or recurrent (cyclic) manner, where
the recurrent variant performs temporal transformations
\footnote{It is possible to \textit{unfold} recurrent
networks to resemble the circular processes until a certain point, achieving
a similar effect to temporal signal transformation, see \cite{Mozer1995}.}
\cite{Schmidhuber2014}.

\begin{equation} \label{eq:ReLU}
f(x) = \begin{cases}
         0 & \text{if } x < 0 \\
	 x & \text{otherwise}
       \end{cases}
\end{equation}

\subsection{Third generation neural networks}
Constructing a network of neuron models essentially creates a non-linear
response to a given numerical vector \cite{Russel2007}.
This transformative view can be adopted to biological (third generation)
\glspl{SNN}, where the data being transferred are no longer vectors, but 
\textit{spikes}\index{neuron!spike}
of electrical current over time \cite[p. 32]{Dayan2001, Eliasmith2004}.

In biological networks there is a temporal dimension, in that neurons
produce and fire spikes asynchronously to other neurons in the same
group \cite{Eliasmith2004}.

Lapicque worked on a conductance model in 1907 that could describe this
process dubbed the \textit{integrate-and-fire} \index{neuron model!integrate-and-fire}
\index{integrate-and-fire|see {neuron model}}
model.
The model essentially integrates received current over time
to determine whether to fire
\cite{Dayan2001, Eliasmith2004}.
The Dirac ($\delta$) function \index{Dirac function}
in Equation \ref{eq:dirac} shows an idealised
version of this idea \cite[p. 404]{Dayan2001}.
For all values it approaches 0, except when its argument is
0 where it will grow infinitely.
In a trial starting at time $0$ and ending at time $t$, this
is the equivalent of summing up the $n$ neuronal events that occurred in 
the duration of the trial.
The total area of these `spikes' sum together to 1 over time $t$. \index{neuron spike}

\begin{equation} \label{eq:dirac}
  \rho(t) = \int_0^T \delta(t) = \sum_{i=1}^n \delta(t - t_i) = 1
\end{equation}

This idealised representation is a common mathematical approximation of
a sequence of activation functions \cite{Dayan2001, Eliasmith2004},
and the foundation for the \textit{third} generation
neural networks, where the computational unit is discrete events over time,
instead of continuous-valued (as in second generation \glspl{NN})
\cite{Maass1997}.\index{neuron spike}\index{spike|see {neuron spike}}

The spiking model is based on a neuron that builds voltage over time, until
it reaches a threshold voltage ($V_{th}$) and emits a spike
\mbox{($\delta(t-t_n)$)}
\cite{Dayan2001, Eliasmith2004}.
The spike carries a charge and is received by a post-synaptic neuron as
input current, which, in turn, decides whether to fire \cite{Dayan2001}.

After spiking the voltage inside the neuron is reset to a value ($\tau_{reset}$),
from which it begins to accumulate charge again.
In a brief period after the activation the neuron enters a period of
refraction, \index{refractory period}
where injected voltage is not accumulated, denoted by 
$\tau_{ref}$, illustrated in Figure \ref{fig:spiking}
\cite[p. 82]{Eliasmith2004}.

A more realistic neuron model was developed by Hodgkin
and Huxley in 1952 dubbed the Hudgkin-Huxley model \cite{Dayan2001}.
While it is more precise than integrate-and-fire models, it is
more complex \cite[p. 195]{Dayan2001} and rarely used in simulations
\cite{Albada2018, Dayan2001, Eliasmith2015}.

\begin{figure}
\centering

\begin{tikzpicture}[scale=3]
\pgfkeys{/pgf/number format/precision=1}
\draw (0,0) -- (2, 0);
\draw (0,0) -- (0, 1);
\foreach \i in {0,0.2,...,1}
  \draw (0,\i) -- (0.02,\i) node [left] {\scriptsize \pgfmathroundtozerofill{\i}\pgfmathresult};
\foreach \i in {0,20,...,100}
  \draw (\i/50,0) -- (\i/50,0.02) node [below] {\scriptsize \i};

\node [rotate=90] at (-0.25, 0.5) {\text{voltage (V)}};
\node at (1, -0.2) {\text{time (ms)}};

\draw [thick] (0,0) .. controls (12/50,0.50) and (36/50,0.6) .. (67/50, 0.639);
\draw [thick] (67/50,0) -- (67/50,1);
\draw [dashed] (0,0.64) -- (2,0.64) node [right] {\scriptsize $V_{th}$};
\node at (67/50, -0.1) {\scriptsize $t_n$};
\draw [thick] (67/50,0) -- ++ (0.1,0);
\node at (80/50,0.85) {\scriptsize $\delta(t-t_n)$};
\draw [thick] (72/50,0) .. controls (81/50,0.25) and (90/50,0.35) .. (2, 0.42);
\node at (75/50,0.3) {\scriptsize $\tau_{ref}$};
\draw (70/50,0.03) .. controls (71.5/50,0.13) .. (74.5/50,0.24);

\end{tikzpicture}
\caption{A model of how a constant, low input current produces a buildup of
	voltage inside a integrate-and-fire neuron, which eventually produces a spike.
	\index{neuron model!integrate-and-fire} 
	After spiking the neuron enters a refractory period, $\tau_{ref}$,
   	where no voltage is integrated.}
\label{fig:spiking}
\end{figure}

Lapicque's model has been elaborated in the \textit{leaky
integrate-and-fire} (LIF) \index{neuron model!leaky-integrate-and-fire}
model, which introduces a numerical ``leak''
into the model, that acts as a type of memory \index{memory}
for the neuron integration \cite{Eliasmith2004, Eliasmith2015}.
In the leaky model, input voltages decays exponentially over time,
meaning that the present voltage depends more strongly on recent input
voltages \cite[p. 85]{Eliasmith2004}.

\begin{equation}
  \frac{dv}{dt} = - {1 \over \tau_{RC}} (v - cr)
  \label{eq:lif}
\end{equation}

The LIF equation is given in Equation \ref{eq:lif}, where
$v$ is the membrane voltage difference between the interior
and exterior of the neuron membrane, $c$ is the input current, $r$
is the ionic current (or leak) of charge across the membrane, and
$\tau_{RC}$ is the membrane time constant that determines how
quickly the neuron decays to its resting state \cite{Dayan2001, Eliasmith2004}.
As the voltage builds up inside the neuron, $r$ will scale the
rate of growth \cite{Eliasmith2004}.
By tuning the leak it is possible to control the time with which
previous voltages are 'forgotten' \cite{Eliasmith2004}.

Similar to second generation neural networks, neurons
receive input from $n$ input neurons.
The connections are commonly referred to as synapses, and are similarly
weighted, as well as translated by a bias (see Equation \ref{eq:weightedoutput}),
such that they contribute differently to the accumulated 
voltage, given by Equation \ref{eq:synaptic} \cite{Dayan2001}.

\begin{equation}
  x_j = \sigma(u_j) = \sum_{i=1}^n{w_i\delta(t - t_i)} + b_j
  \label{eq:synaptic}
\end{equation}

\subsection{Coding spikes}
Spikes convey information in the form of amplitude, duration, and
inter-spike intervals \cite{Dayan2001}.
A number of methods exist to decode the information in the spikes, by a 
combination of the three parameters \cite{Dayan2001, Eliasmith2015, Diehl2015, Rueckauer2017},
but this thesis will
focus on so-called rate models\index{rate encoding|see {encoding}}\index{encoding!rate}.
Recording neuron spikes over time provides an array of timestamps called a
spike train\index{spike train} \cite{Eliasmith2015}.
Rate models simply count the timestamps and divide them by the duration of
the trial to produce the spike \textit{rate}, shown
in Equation \ref{eq:spikerate} \cite{Dayan2001, Eliasmith2004}.
Semantically this is the equivalent of averaging the number of spikes propagated
in that interval, and provides the basis for a numerical interpretation of a
neuron's output \cite{Eliasmith2004}.

\begin{equation}
  r = {n \over T} = {n \over \rho(t)} = {1 \over T} \int_0^T \delta(t)
\label{eq:spikerate}
\end{equation}

When encoding numerical information to spikes, it is useful to 
express the spikes stochastically, such that one scalar determines
the probability that a neuron spikes over time \cite{Dayan2001}.
Assuming that the spikes are independent from each other, this
probability can be expressed by a probability distribution such as the
Poisson distribution\index{Poisson distribution}
\cite{Dayan2001}.
The Poisson distribution determines the probability of a number
of events occurring in an interval, given that the events are
known to happen at a fixed rate \cite{Dayan2001}.
It is defined in Equation \ref{eq:poisson} where $n$ is the
number of events and $\lambda$ is the rate with which events
happen \cite{Dayan2001}.
Figure \ref{fig:poisson} shows the probability that the number of
observed events ($k$) matches the poisson rate ($\lambda$).

\begin{equation}
P(n) = \lambda^n {e^{-\lambda} \over n!}
\label{eq:poisson}
\end{equation}

\begin{figure}
\centering
\begin{tikzpicture}
\pgfmathdeclarefunction{poiss}{1}{%
  \pgfmathparse{(#1^x)*exp(-#1)/(x!)}%
}
\begin{axis}[ylabel={P(X=k)},xlabel={k},legend
    entries={$\lambda=1$,$\lambda=4$,$\lambda=7$},every axis plot post/.append style={
    samples at = {0,...,15},
    axis x line*=bottom,
    axis y line*=left,
    enlargelimits=upper}]
  \addplot +[sharp plot] {poiss(1)};
  \addplot +[sharp plot] {poiss(4)};
  \addplot +[sharp plot] {poiss(7)};
\end{axis}
\end{tikzpicture}
\caption{The probability of a number of events ($k$) occurring, given the
poisson rates ($\lambda$) of 1 (blue), 4 (red) and 7 (green).}
\label{fig:poisson}
\end{figure}

To align digital representations with neural spikes,
signals are encoded and decoded when entering and leaving the \gls{SNN}
\cite{Dayan2001}.
To compare between non-spiking and spiking networks is it necessary to 
provide a coding scheme that transfers between the two representations.

Given the stepwise activation function from Equation \ref{eq:perceptron},
the neuron can be said to spike with \cite[p. 3]{Diehl2015, Rueckauer2017}:

\begin{equation}
\Theta(x) = \begin{cases}
  	1 & \text{if } x \geq 0 \\
	0 & \text{otherwise}
	\end{cases}
\label{eq:spike_theta}
\end{equation}
\noindent
In turn the occurrence of a spike for a neuron $i$ at timestep $t$ can be
calculated by the integration of input current at every simulation time step, where
$v_i^l$ is the membrane potential for the neuron \cite[p. 3]{Rueckauer2017}:

\begin{equation}
\Theta^l_{t,i} = \Theta(v^l_i (t-1) + \zeta^l_i(t) - V_{thr}),
\label{eq:current_timestep}
\end{equation}
\noindent
Recalling that a neuron $i$ at layer $l$ receives post-synaptic
impulses from $j$ neurons from layer $l - 1$, the input current $\zeta^l_i$ for neuron $i$
at layer $l$ can be seen as the linear equation
\cite[p. 3]{Rueckauer2017}:

\begin{equation}
\zeta^l_i (t) = V_{thr}\left(\sum^{N^{l - 1}}_{j = 1} w^l_{ij} \Theta^{l - 1}_{t,j} + b^l_i\right) ,
\label{eq:membrane_timestep}
\end{equation}
\noindent
where $V_{thr}$ is the neuron threshold. 

Neurons integrate $\zeta_i^l(t)$ until the threshold $V_{thr}$ is 
reached, where a spike is emitted and the membrane potential is reset to 
0 (see Figure \ref{fig:spiking}).
The membrane current $v_i^l(t)$ can then be modelled as

\begin{equation}
v_i^l(t) = \left(V_i^l(t - 1) + \zeta_i^l(t)\right)\left(1 - \Theta_{t,i}^l\right)
\label{eq:membrane_current_sim}
\end{equation}

\noindent
Assuming that the input current is above zero ($\zeta_i^1 > 0$) and that it
remains constant through time, there will be a constant number of timesteps $n_i^1$
between spikes in the neuron $i$, and the neuron threshold will always be exceeded
by the same amount:

\begin{equation}
\epsilon_i^l = v_i^1(n_i^1) - V_{thr} = n_i^1 \cdot \zeta_i^1 - V_{thr}
\label{eq:threshold}
\end{equation}
\noindent
Assuming the same constant input current $\zeta$ such that $\sum_{t'}^t 1 = t/\Delta t$, 
and realising that the number of spikes $N$ in a simulation of duration $t$ is
$N(t) = \sum_{t'=1}^t\Theta_{t'}$, 
the membrane potential can be obtained by summing over the simulation duration $t$
\cite{Rueckauer2017}:
\begin{equation}
\begin{split}
\sum_{t'}^tv(t') &= \sum_{t'=1}^t v(t' - 1)(1 - \theta_{l'}) + (1 - \Theta_{t'}) \\
     		 &= \sum_{t'=1}^t v(t' - 1)(1 - \theta_{l'}) + \zeta({t \over \Delta t} - n) \\
\end{split}
\label{eq:sum_rate}
\end{equation}
\noindent
The layer and neuron indices are omitted for clarity.

By further rearranging Equation \ref{eq:sum_rate} and defining the maximum firing rate 
as $r_{max} = 1 / \Delta t$, the average firing rate $N/t$ can now
be calculated by dividing with the simulation time \cite{Rueckauer2017}:

\begin{equation} 
\begin{split}
{1 \over \zeta t} \sum_{t'}^tv(t') &= {1 \over \zeta t} \sum_{t'=1}^t v(t' - 1)(1 - \Theta_{l'}) + \zeta({t \over \Delta t} - N) \\
{1 \over \zeta t} \sum_{t'}^tv(t') &= {1 \over \Delta t} - {N \over t} + {1 \over \zeta t} \sum_{t'=1}^t v(t' - 1)(1 - \Theta_{l'}) \\
{1 \over \zeta t} \sum_{t'}^tv(t') + {N \over t} &= r_{max} + {1 \over \zetat} \sum_{t'=1}^t v(t' - 1)(1 - \Theta_{l'}) \\
{N \over t} &= r_{max} + {1 \over \zeta t} \sum_{t'=1}^t \left(v(t' - 1)(1 - \Theta_{l'}) - v(t')\right) \\
r = {N \over t} &= r_{max} - {1 \over \zetat} \sum_{t'=1}^t \left(v(t') - v(t' - 1)(1 - \Theta_{l'})\right) \\
\end{split}
\label{eq:spike_rate_sum}
\end{equation}

\noindent
Since the input current is constant, the value of the membrane potential before a spike is always the same,
and is always an integer multiple of the input $\zeta$.
Defining $n \in \mathbbm{N}$ as the number of simulation steps needed to cross the threshold $V_{thr}$, then

\begin{equation}
{1 \over \zetat} \sum_{t'}^tv(t' - 1)\Theta_{t'} = {1 \over \zetat}(n - 1)\zetaN = r(n - 1)
\label{eq:time_threshold}
\end{equation}

\noindent
Realising that

\begin{equation}
\sum_{t'=1}^tv(t') - v(t' - 1) = v(t) - v(0)
\label{eq:sum_index_shuffle}
\end{equation}

\noindent
Equation \ref{eq:spike_rate_sum} simplifies to:

\begin{equation}
\begin{split}
r    &= r_{max} - {1 \over \zeta t} \sum_{t'=1}^t \left(v(t') - v(t' - 1)(1 - \Theta_{l'})\right) \\
     &= r_{max} - {v(t) - v(0) \over \zeta t} - {1 \over \zeta t} \sum_{t'=1}^t v(t' - 1)\Theta_{l'}\\
     &= r_{max} - {v(t) - v(0) \over \zeta t} - r(n - 1)\\
     &= r_{max} - {v(t) - v(0) \over \zeta t} - rn - r\\
r &= {1 \over n} \left(r_{max} - {v(t) - v(0) \over \zeta t}\right)
\end{split}
\label{eq:spike_rate}
\end{equation}

Finally, the residual charge $\epsilon \in \mathbbm{R}$ is defined as the surplus charge at the time of
a spike:

\begin{equation}
\epsilon = n\zeta - V_{thr}
\label{eq:charge_surplus}
\end{equation}

\noindent
and remembering that the first layer at constant input $\zeta^l = V_{thr}x^1$, the average spike rate
for that layer can now be defined with re-introduced neuron and layer indices:
\begin{equation}
\begin{split}
r_i^1(t) &= {1 \over n_i^1} \left(r_{max} - {v_i^1(t) - v_i^1(0) \over \zeta t}\right) \\
         &= {1 \over {(\epsilon_i^1 + V_{thr}) / \zeta}} \left(r_{max} - {v_i^1(t) - v_i^1(0) \over \zeta t}\right) \\
         &= {\zeta \over \epsilon_i^1 + V_{thr}} \left(r_{max} - {v_i^1(t) - v_i^1(0) \over \zeta t}\right) \\
         &= {V_{thr} x_i^1  \over \epsilon_i^1 + V_{thr}} r_{max}
          - \left({\zeta \over \epsilon_i^1 + V_{thr}} {v_i^1(t) - v_i^1(0) \over \zeta t}\right) \\
         &= x_i^1 r_{max} {V_{thr} \over V_{thr} + \epsilon_i^l } - {v_i^1(t) \over t (V_{thr} + \epsilon_i^l) } \\
%x_i^1 r_{max} \cdot {v_{thr} \over v_{thr} + \epsilon_i^1} - {v_i^1(t) \over t \cdot (v_{thr} + \epsilon_i^l)} \\
\end{split}
\label{<+label+>}
\end{equation}

as argued by \citeauthor{rueckauer2017} the membrane reset discards the additional
information in the $\epsilon$ term.
for that reason

assuming that the poisson rates represents an averaging of spikes over time,
the average firing rate $r_i^l(t)$ of some neuron $i$ in layer $l$ can be computed
by summing the number of spikes kor the duration of the simulation $t$ \cite{rueckauer2017}:

\begin{equation}
\sum_{t=1}^t \rho(t) = 
\label{fig:lif-rate}
\end{equation}

for a lif neuron the frequency of spikes over time, given a variable input current,
is given by \cite[p.87]{eliasmith2004}

assuming that the stochastic process smoothes out over time, this equation
relates input in \glspl{ANN} to input in \glspl{SNN}.


\section{Learning} \index{learning} \label{sec:learning}
% TODO: http://www.cs.toronto.edu/~fritz/absps/momentum.pdf
Defining an \gls{agent} as a system that can act on previous knowledge
\cite{Russel2007}, learning in the context of an \gls{agent}
refers to ``the process of gaining
information through observation'' \cite{sep:learning-formal}.

Following the above abstraction of neural networks as computations over vectors,
``learning'' \index{learning} can be understood as the development of consistent
patterns, given the same input.
Within the \gls{ml} literature, this is commonly referred to as 
\textit{prediction}. \index{learning!prediction}
In practice this is expressed in terms of general functions or
\textit{rules} \index{rule} in a network \cite[p. 704.]{Russel2007}.

Within machine learning \gls{ml} three types of
learning exists: supervised, \index{learning!supervised}
unsupervised \index{learning!unsupervised} and reinforced.
\index{learning!reinforcement}

\textit{Supervised} learning relies on a set of expected outputs which 
the learning \gls{agent} must predict, given some input \cite{Russel2007}.
The \gls{agent} is told how `wrong' it was, so it can adapt accordingly.
This learning typically happens in a \textit{training} and \textit{testing}
phase, where the \gls{agent} is allowed to builds its internal representation
which is later tested previously unseen data \cite{Russel2007}.

\textit{Unsupervised} learning asks the \gls{agent} to learn without
having any idea of error margin \cite{Russel2007}.
Rather, the \gls{agent} is asked to \textit{explore} a domain in search of
patterns, which then form the basis for future predictions or classifications
\cite{Russel2007}.

\textit{Reinforced} learning reinforces the \gls{agent} through
rewards and discourages it through punishments \cite{Russel2007}.
Contrary to supervised learning the rewards and punishments are not
instructing the agent on what the output should be, but rather how well
it performed the task, leaving the \gls{agent} to infer rules or
behaviours by itself \cite[p. 873]{Russel2007}.

The process of learning can either be \textit{inductive}
\index{learning!induction}
or \textit{deductive} \index{learning!deduction} \cite[p. 704]{Russel2007}.
The latter requires a basis in rule-based systems from which new knowledge can
be deduced, while the former requires a measurement of success \cite[p. 705]{Russel2007}.
Such a measurement is typically referred to as the \textit{error} or \textit{loss}
function,\index{loss function}\index{error function|see {error function}}
because it shows how much the prediction deviated from the expectation (goal)
\cite{Russel2007}.

Learning within neural networks have shown to be possible within all three
types of learning \cite{Schmidhuber2014, Russel2007}, but deduction is rarely
seen in the \gls{NN} literature, because it is cumbersome to express neural
networks through logic transformations \cite{Pearl2018}.

Because of its simplicity and widespread use, this thesis will focus on supervised
inductive learning.

\subsection{Errors in learning}
It is worth noting that \gls{NN}s may learn \index{learn}
rules \index{learning!rule} that are not optimal \cite{Russel2007}.
This can happen in one of two ways: either the
network is structurally incapable of learning the domain, 
or the learned rule is incorrect \cite{Russel2007, Eliasmith2015}.

An \gls{NN} is limited in complexity by its number of nodes,
because one neuron is only as complex as its activation function
\cite{Dayan2001, Russel2007}.
Such a structural limit cannot be solved by any other means
than augmenting the network \cite{Russel2007}.

Seeing neural networks as complex non-linear systems, with
a number of parameters for the weights and biases,
the network can be visualised as a point in a high-dimensional space 
\cite{Russel2007}. 
Provided that the network is sufficiently complex, the 
learned rules can still fail to achieve a good accuracy because
the system falls into local minima \cite{Russel2007}.

A similar problem occurs when the model only trains on data
that is not representable for the more general domain \cite{Russel2007}
This type of ``overfitting'' can be avoided if the network
is only trained on parts of the available data, and to test it on 
the remaining data \cite{Russel2007, Schmidhuber2014}.
The limits between data for training and data for testing
are not agreed upon, but a 80\%/20\% split seems to be
the default \cite{Russel2007, Schmidhuber2014}.

\subsection{Backpropagation}
In the search for optimal weight/bias configurations,
such that prediction errors are minimised \cite{Rumelhart1988},
the network weights constitutes the search space, 
and the loss function \index{loss function}
is the subject of the optimisation \cite{Russel2007}.

One loss function to minimize for a supervised network
is described in equation \ref{eq:bp-loss},
where the actual output $x$ is compared to the 
target (desired) output $t$, for all output neurons $n$
\cite{Russel2007}.

\begin{equation}
  E = {1 \over 2} \sum_{i=1}^n |x_i - t_i|^2
  \label{eq:bp-loss}
\end{equation}

As has been shown, feedforward networks perform this calculation
through the sequential application of the weighted activation
functions in each layer.
One method to minimising this error is
to calculate the gradients of the network layers and iteratively walk in
the opposite direction of the error, a technique called gradient
descent\index{gradient descent}\cite{Rumelhart1988, Russel2007}.
Gradient descent requires that the network
activation functions are differentiable,
such that the gradient of $E$ with respect to the layer weights
($w_1 \cdots w_l$) can be found, and be iteratively
adjusted \cite{Rojas1996}, as shown in equation \ref{eq:bp-weights}.

\begin{equation}
  \Delta E = (
    {\partial E \over \partial w_1}, 
    {\partial E \over \partial w_2},
    \cdots, 
    {\partial E \over \partial w_l}
  )
  \label{eq:bp-weights}
\end{equation}

Deriving the error function from equation \ref{eq:bp-loss} gives

\begin{equation}
  \delta_j = y_j - t_j
  \label{eq:bp-loss-prime}
\end{equation}

The derivation of $E_n$ for an output neuron $n$ from layer $i$ to layer $j$
depends on the weight $w_{ji}$ via the activation input $u_j$ (see equation
\ref{eq:weightedoutput}).
We can therefore apply the chain rule for partial derivatives \cite{Bishop2006}:

\begin{equation}
  {\partial E_n \over \partial w_{ji} } =
  { \partial E_n \over \partial u_j}
  {\partial u_j \over \partial w_{ji}}
  \label{eq:bp-chain}
\end{equation}

and replace the following terms

\begin{equation}
  \delta_j = 
  { \partial E_n \over \partial u_j}
  \qquad
  z_i = { \partial u_j \over \partial w_{ji}}
  \label{eq:bp-d}
\end{equation}

to get 
\begin{equation}
  {\partial E_n \over \partial w_{ji} } =
  { \partial E_n \over \partial u_j}
  {\partial u_j \over \partial w_{ji}} = 
  \delta_j z_i
  \label{eq:bp-chain2}
\end{equation}

Observing that $\delta_j$ can be further expanded to the sum of all units $k$
that receives input from unit $j$ such that

\begin{equation}
  \delta_j = {\partial E_n \over \partial u_j} =
  \sum_k {\partial E_n \over \partial u_k} 
  {\partial u_k \over \partial u_j}
  \label{eq:bp-d2}
\end{equation}

and substituting \ref{eq:bp-d} into \ref{eq:bp-d2} while applying the chain rule, such that

\begin{equation}
  \begin{split}
  \delta_j & = \sum_k {\partial E_n \over \partial u_k} 
   {\partial u_k \over \partial u_j} \\
   & = \sum_k \delta_k {\partial u_k \over \partial \sigma_j} {\partial \sigma_j \over \partial u_k} \\
   & = \sum_k \delta_k w_{kj} \sigma'_j \\
   & = \sigma'_j \sum_k w_{kj} \delta_k 
  \end{split}
  \label{eq:bp}
\end{equation}

This more general form of backpropagation can be chained through layers, where
the outer most $\delta_j$ term---also called the layer ``error''---is defined 
as in \ref{eq:bp-loss-prime}.

In each layer weights are updated as shown in equation \ref{eq:bp-update},
to approach an optimal configuration.
For biases the derivative $\partial a_j \mathbin{/} \partial b_j = 1$ such that bias
updates are given by \ref{eq:bp-update-bias}.
Here $\gamma$ is a factor that controls the speed with which the weights
are corrected (learning rate). \index{learning rate}

\begin{equation}
  \Delta w_i = -\gamma {\partial E \over \partial w_i}
  \label{eq:bp-update}
\end{equation}

\begin{equation}
  \Delta b_i = -\gamma \delta_j
  \label{eq:bp-update-bias}
\end{equation}

The learning rate exists to avoid ``shooting over'' optimal points
\cite{Russel2007}.
Typically a momentum is added to the learning rate, depending on the norm of
$\delta_j$, such that see \cite{Sutskever2013, LeCun1998}. 
Several other techniques have been invented to circumvent the problem of 
local minima and optimise the learning of the model \cite{LeCun1998,
Schmidhuber2014}, but they will not be covered here.

\section{Neuromorphic computing}
Neuromorphic hardware is based on the idea of \gls{NN}s where the activation
units are modelled outside the classical \gls{vonNeumann} architectute, either in 
integrated circuits or in simulated environments \cite{Albada2018,Blundell2018,Schmitt2017}.

This approach permits the simulators to work several hundred of magnitudes
faster than regular \gls{ANN}s, but at the cost of precision and noise
\cite{Indiveri2015, Schmitt2017}.
The precision problems occur because of hardware limitations where the typical
weight is restricted to a few bits, compared to larger \gls{ANN} networks
\cite{Indiveri2015, Lin2018}.
The noise problems are caused by noise in the integrated circuit components 
\cite{Lin2018, Pfeil2013}.

Furthermore the technology is still relatively young and suffers from a number
of practical problems, why networks above a couple of thousand neurons remains
a problem in the available platforms \cite{Schmitt2017}.

A number of examples are given in section \ref{sec:similar-neuromorphic}.

%TODO: Add section on REF and the need for parallel networks

\end{document}
