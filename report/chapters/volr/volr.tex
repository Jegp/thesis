This sections presents the \gls{DSL} Volr. \index{Volr}
The main purpose of Volr is to define clear and reproducible
experiments whose semantics are retained regardless of
the runtime environment.
The specification in its current form is relatively simple, but sufficiently 
complicated for the purpose of this thesis.
It focuses solely on the topology of networks, thus
separating the network description from any generation-specific properties
of neurons or neuron populations.

The first requirement is achieved through an unambiguous syntax inspired
by the lamdba calculus \cite{Pierce2002}.
Figure \ref{fig:volr-expr} shows the BNF notation for expressions, values and types
in Volr. 
Figure \ref{fig:volr-rules} lists evaluation rules for the correct
interpretation of the expressions.

\input{chapters/volr/expr.tex}
\input{chapters/volr/evaluation.tex}

The constant expression $n$ is simply an integer that evaluates to the type 
\texttt{\textbf{int}} ($e1$). 
Similarly to the lambda calculus, the \texttt{\textbf{let}} binding binds
the string constant $x$ to the expression $e$ in an encapsulated
environment $e'$ \cite{Pierce2002}.
That constant can later be referenced in the $e'$ expression 
through the string $x$ as shown in $e2$.

The \texttt{\textbf{dense}} expression describes a network of two
populations, and is the most basic concept in the \gls{DSL}.
Notice the distinction between a neural network \textit{layer}
such as \texttt{\textbf{dense}} and a population. 
In a \texttt{\textbf{dense}} network layer, every neuron from the
first population is connected to every neuron in the second population
(\textit{densely} or all-to-all).
The two parameters $n$ and $m$ defines the number of neurons in the first and 
second layer respectively, and evaluates to the \texttt{\textbf{net}}
fundamental network type and value, as shown in $e3$. 
Considering how each neuron is a type of classifier, these numbers
illustrate the \textit{dimensionality} of the network, such that the number of
dimensions in the input is truncated (or expanded) to classify the
dimensionality of the output layer.

The $\obar$ (sequential) operator binds two networks sequentially,
such that the output layer of the first network becomes the input layer 
of the second network.
The two networks $e$ and $e'$ will share one of the neuron populations, why
the output size of the first layer is required to be equal to the input
size of the second layer ($e4$).

The $\ominus$ (parallel) operator parallelises two networks by duplicating
the input from the previous layer and merging the outputs into a single
layer ($e5$).
The input feeds into both $e$ and $e'$, such that the input dimension of
the network must be shared by the two layers ($l$). 
The output from the network is stacked such that each neuron from each
population corresponds to one output neuron ($e_{out} + e'_{out}$).
This is done to preserve the meaning of each parallel population.

Taken together these constructs can express simple neural networks and
the properties of their connections. 
Figure \ref{fig:volr-examples} shows a number of example networks
that visualises four examples of networks. 

\input{chapters/volr/example.tex}

\FloatBarrier
