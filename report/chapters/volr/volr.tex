This sections presents the implementation for the Volr \gls{DSL}. \index{Volr}
The main purpose of Volr is to define clear and reproducible
experiments whose semantics are retained regardless of
the runtime environment.
It is built for the concise specification and straightforward
translation of neural models into both artificial, as well as
spiking, network models.
Volr focuses solely on the topology of networks, thus
separating the network description from generation-specific
properties of neurons or neuron populations.

The first requirement on semantic consistency explained in the previous section is met through an unambiguous
syntax, heavily inspired by lambda calculus \cite{Pierce2002}.
Figure \ref{fig:volr-expr} shows the BNF notation for expressions, values and types
in Volr. 
Figure \ref{fig:volr-rules} lists typing rules for the correct
interpretation of the expressions.

\input{chapters/volr/expr.tex}
\input{chapters/volr/typing.tex}

The constant expression $n$ is an integer that evaluates to the type 
\texttt{\textbf{int}} ($e1$). 
Similar to traditional functional languages, the \texttt{\textbf{let}} binding
binds the string constant $x$ to the expression $e$ when evaluating
$e'$ \cite{Pierce2002}.
That can later be referenced in the $e'$ expression 
through the string $x$ as shown in $e2$.

The \texttt{\textbf{dense}} expression describes a fully connected
neural network layer, and is the smallest building block in the language.
This aligns with the previous understanding of a layer, where
a \texttt{\textbf{dense}} network layer can be understood as a number of inputs,
that are densely connected to a number of outputs (see Section
\ref{sec:nn_graphs}).
To calculate the output of the layer, the layer bias is added to the
weighted input (output from the previous layer) and given 
to the activation function.
Taking into account the definition of layers as functions over vectors as described previously, each neuron 
accounts for the activation of a single dimension in the input/output vectors.
In turn, $n$ and $m$ illustrate the \textit{dimensionality} of the network,
such that the number of dimensions in the input is truncated (or expanded) to
the dimensionality of the output layer.

The $\ominus$ (parallel) operator parallelises two networks by duplicating
the input from the previous layer and merging the outputs into a single
layer ($e5$).
The input from the previous layer is replicated into both $e$ and $e'$, such
that the input dimension of the network \textit{must} be shared by the two layers ($l$).
The output from the network is stacked such that each neuron from each
population corresponds to one output neuron ($e_{out} + e'_{out}$).
This is done to preserve the meaning of each parallel population, where
a truncation would loose information.

Semantically the parallel operator provides the ability for a network to
perform specialised functions, based on the same stimulus.
In the context of neural systems such specialisations are frequently used
to balance correctness with compactness: it is cheaper in terms of neurons,
and more efficient in terms of accuracy,
to allow two subnetworks to specialise than to have one large
generalised network \cite{Eliasmith2015}.

The $\obar$ (sequential) operator binds two networks sequentially,
such that the output layer of the first network becomes the input layer 
of the second network.
This binding is similar to the \texttt{\textbf{dense}} operation before,
in that the neurons are connected densely, but differ because the operator
can connect parallel networks.
To connect networks sequentially, it is expected that the output dimensionality of
the first network ($m$) equals that of the input of the second network, as shown in $e4$.

Taken together these constructs can express simple neural networks and
the properties of their connections. 
Figure \ref{fig:volr-examples} shows a number of example networks
that visualises four examples of networks. 

\input{chapters/volr/example.tex}

\FloatBarrier
