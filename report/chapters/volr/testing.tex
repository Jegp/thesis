To verify that the DSL implementation was successful, and that the models perform as expected when evaluated on the
backends, a number of tests were written and automated.
This sections elaborates on the reasoning and design of the tests, which are divided into two categories: unit
tests and integration tests.
\\[0.2cm]
All tests are available online, see Appendix \ref{app:implementation}.

\subsection{Unit tests}
\subsubsection{Volr compiler}
Each expression construct in the compiler---and their combinations---are
tested as to whether the expected output is produced, such that the evaluator
is guaranteed to generate well-formed code.
An example is shown in Listing \ref{lst:eval-test},
in which a unit test \index{unit test} verifies that the let
binding of the constant \texttt{x} correctly
evaluates to the network \texttt{(\textbf{dense} 1 1)}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=haskell,caption={Part of the evaluation code in
Haskell.},label={code:evaluator}]
eval' :: Term -> EvalState Term
eval' term =
  case term of
    TmNet n m -> return $ TmNet n m
    TmSeq t1 t2 -> do
      t1' <- eval' t1 
      t2' <- eval' t2
      return $ TmSeq t1' t2'
\end{lstlisting}
\end{minipage}

\begin{lstlisting}[language=Haskell,label={lst:eval-test},caption={A unit test for the correct evaluation of a let binding.}]
it "can evaluate a let binding with a reference" $ do
  let e = TmLet "x" (TmNet 1 1) (TmRef "x")
  eval e `shouldBe` Right (TmNet 1 1)
\end{lstlisting} 

\subsubsection{Futhark backend}
The Futhark backend was tested using unit tests (using \texttt{futhark-test}
\cite{Elsman2018}) for each layer, activation function and loss function.
Tests for the dense layer already existed in the library used (\cite{Minh2018}). Tests for the merge and
replicate layers were added using manual calculations of the expected gradients, as shown in Listing
\ref{lst:futhark-replicate-test}.The input should produce the expected weight
gradients (\texttt{output}) here.
Further tests for the different combinations of parallel layers were also added.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=,label={lst:futhark-replicate-test},caption={A test for the
correct calculation of the backwards weight gradient during backpropagation in a
replicate layer}]
-- ==
-- entry: replicate_backward_w
-- input {[[1.0, 2.0, 3.0, 4.0],
--         [2.0, 3.0, 4.0, 5.0],
--         [3.0, 4.0, 5.0, 6.0]]
--
--        [[1.0,  2.0,  3.0,  4.0],
--         [5.0,  6.0,  7.0,  8.0],
--         [9.0, 10.0, 11.0, 12.0]]
--
--         [1.0, 2.0, 3.0]}
--
-- output {[[-25.60,  -36.90,  -48.20,  -59.50],
--          [-59.00,  -87.40, -115.80, -144.20],
--          [-92.40, -137.90, -183.40, -228.90]]}

entry replicate_backward_w input w b =
  let ws = ((w, b), (w, b))
  let (cache, output) = replicate.forward true ws input
  let (_, ((w',_), (_, _))) = replicate.backward false updater ws cache output
  in w'
\end{lstlisting}
\end{minipage}

\subsubsection{PyNN}
The backend-agnostic code of PyNN was tested using the \texttt{Pytest} framework
\cite{Pytest2018}.
This includes test for the activation functions, spike normalisation functions, error functions, 
as well as general Python structure.

Especially the model class in the PyNN backend contains error-prone code, because it
deals with stateful backends.
Unit tests are therefore particularly crucial.
The test shown in Listing \ref{lst:pynn-model-test} shows an example of such a
test that, in this case, validates that the simulator is properly reset between runs:

\begin{lstlisting}[language=Python,label={lst:pynn-model-test},caption={Unit test for
PyNN model to validate that the model correctly updates weights}]
def test_nest_model_backwards_reset():
    p1 = pynn.Population(2, pynn.IF_cond_exp())
    p2 = pynn.Population(2, pynn.IF_cond_exp())
    l1 = v.Dense(p1, p2, v.ReLU(), decoder = v.spike_count_normalised, weights = 1)
    m = v.Model(l1)
    xs1 = np.array([1, 1])
    ys1 = np.array([0, 1])
    xs2 = np.array([1, 1])
    ys2 = np.array([0, 1])
    # First pass
    target1 = m.predict(xs1, 50)
    m.backward([0, 1], lambda w, g, b, bg: (w - g, b - bg))
    expected_weights = np.array([[1, -1], [1, -1]])
    assert np.allclose(l1.get_weights(), expected_weights)
    # Second pass
    target2 = m.predict(xs2, 50)
    m.backward([1, 0], lambda w, g, b, bg: (w - g, b - bg))
    expected_weights = np.array([[-1, -1], [-1, -1]])
    assert np.allclose(l1.get_weights(), expected_weights)
\end{lstlisting}

The PyNN code for the NEST backend was also tested with \texttt{Pytest}
\cite{Pytest2018}.
A unit test for the backpropagation was written using numerical gradient descent
with a simulated feedforward step.
In the test, the layer weights are slightly skewed to approximate the movement
along a gradient, which, in the test, was based on a sigmoid function.
The resulting changes in the backpropagated error should be minuscule, provided
that the algorithm has been implemented correctly.
A snippet of the test is shown below.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Python,label={lst:volrpynn_numerical},caption={Part
of the numerical gradient test for the densely connected layer in PyNN.}]
# Calculate numerical gradients
numerical_gradients = compute_numerical_gradient(xs, ys)
# Calculate 'normal' gradients
gradients = compute_gradients(xs, ys)
# Calculate the ratio between the difference and the sum of vector norms
ratio = np.linalg.norm(gradients - numerical_gradients) /\
           np.linalg.norm(gradients + numerical_gradients)
assert ratio < 1e-07
\end{lstlisting}
\end{minipage}

\subsubsection{Continuous integration}
Continuous integration (CI) is a tool to automatically trigger the unit tests, and is
typically associated with updates in a version control system. \index{Continuous
integration}
The projects in this thesis are all exploiting this to continually verify that
the software does not regress.
Whenever changes are committed and published to their respective repositories on
GitHub, the unit tests are executed.
Unit tests for NEST are, however too computationally expensive for the
continuous integration service due and are left out.

\subsection{Integration tests}
Integration tests exists to verify that the entire toolchain is functioning.
It is not intended to test the correctness of the individual parts, but rather
that they correctly integrate with each other.
The tests are performed inside a containerised environment, as a
method to ensure a homogeneous environment. 

Integration tests have been performed during the construction of the software,
but no automated tests are in place.

The tests assert that the \gls{DSL} compiler is capable of compiling the models
into code for the individual backends that executes and provides the correctly
formatted result.
The actual values are not verified, because the integration tests assume that
the projects are well-behaved independently.
