The compiler for the \gls{DSL} itself is implemented in the functional language
Haskell.
The compiler generates code for two different backends: a second generation
backend in Futhark \index{Futhark} and a third generation backend in PyNN.
\index{PyNN}.

Futhark has been chosen because it is uncommonly concise and offers
useful functional abstractions that cleanly composes functional
models \cite{Henriksen2017}.
Considering \gls{NN}s as a structure of feedforward and feedback functions,
Futhark is an elegant solution for the task.

PyNN has been chosen for its general purpose API that translates to both the
neuromorphic BrainScaleS \index{BrainScaleS} backend and the NEST 
\index{NEST} simulator.
\\[0.2cm]
Figure \ref{fig:volr-architecture} shows the workflow from the compiler
down to each backend.
The following section explains the diagram one component at the time.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{images/volr-architecture.png}
  \caption{The workflow from the Volr compiler to 
    \gls{ANN} simulations in Futhark or \gls{SNN} simulations in \gls{NEST} and
    \gls{BrainScaleS}.
  }
  \label{fig:volr-architecture}
\end{figure}

\subsection{DSL compiler}
The Haskell compiler consists of five different parts:
An \gls{AST}, evaluator, language parser and two
code generators for \gls{ANN}s and \gls{SNN}s.

\lstset{caption=The Volr AST in Haskell., mathescape=false,showstringspaces=false}
\begin{lstlisting}{haskell}
data Term
  = TmNet Int 
  | TmPar Term Term                                                                                                                                                                                             
  | TmRef String                                                                                                                                                                                                
  | TmLet String Term Term                                                                                                                                                                                      
  deriving (Eq, Show)  
\end{lstlisting} \label{code:term}

The AST reflects the expressions given in the 
specification (section \ref{sec:volr}) and is
shown in listing \ref{code:term}.
It is accompanied by a simple type system (in listing \ref{code:type})
that similarly maps to the types given above.

\lstset{caption=Volr type system in Haskell.}
\begin{lstlisting}
data Type 
  = TyNetwork Int Int
  | TyInt
\end{lstlisting} \label{code:type}

The evaluator component evaluates the expression tree into
a reference-free model, checking the type integrity in the
process.
Each expression construct---and their combinations---are
tested to produce the expected output, such that the evaluator
is guaranteed to output well-formed code.
An example of this is shown in listing \ref{code:eval-test}
where a unit test \index{unit test} written in \texttt{HaskellSpec} 
tests whether the let binding of the constant \texttt{x} correctly
evaluates to the network \texttt{Net 1 1}.

\lstset{caption=Part of the evaluation code in Haskell.}
\begin{lstlisting}
eval' :: Term -> EvalState Term
eval' term =
  case term of
    TmNet n m -> return $ TmNet n m
    TmSeq t1 t2 -> do
      t1' <- eval' t1 
      t2' <- eval' t2
      return $ TmSeq t1' t2'
\end{lstlisting} \label{code:evaluator}

\lstset{caption=A unit test for the correct evaluation of a let binding.}
\begin{lstlisting}{code=Haskell}
it "can evaluate a let binding with a reference" $ do
  let e = TmLet "x" (TmNet 1 1) (TmRef "x")
  eval e `shouldBe` Right (TmNet 1 1
\end{lstlisting} \label{code:eval-test}

Listing \ref{code:eval-type} shows the type checking
that also occurs in the evaluator step. 
If the model is malformed, an error is output to explain why the
model could not evaluate correctly.
Tests for the type checks and the evaluator in its entirety
can be found in appendix \ref{app:implementation}.

\lstset{caption=Part of the type checking code in Haskell.}
\begin{lstlisting}{language=Haskell}
typeOf :: Term -> EvalState Type
typeOf term = 
  case term of
    TmNet n m -> return $ TyNetwork n m
    TmSeq t1 t2 -> do
      leftOut <- sizeRight t1
      rightIn <- sizeLeft t2
      if leftOut == rightIn then do
        leftIn <- sizeLeft t1
        rightOut <- sizeRight t2
        return $ TyNetwork leftIn rightOut
      else
        throwError $ "Type error: Incompatible network sizes. " ++
                     "Output " ++ (show leftOut) ++ " should " ++
                     "be equal to input " ++ (show rightIn)
\end{lstlisting} \label{code:eval-type}

The language parser interprets textual input into the AST. \index{AST}
The parser is built via the monadic parser combinator library Megaparsec
\cite{megaparsec}.
The main component of the parser is shown in listing \ref{code:parser}
and the whole parser is available in appendix \ref{app:implementation}.

\begin{lstlisting}{language=Haskell,name=The main component of the text parser for Volr.}
parseTerm :: Parser Term
parseTerm = (lexeme $ choice
  [ TmNet <$> (symbol "Net" *> integer) <*> integer
  , TmPar <$> (symbol "Par" *> (parens parseTerm)) <*> (parens parseTerm)
  , TmSeq <$> (symbol "Seq" *> (parens parseTerm)) <*> (parens parseTerm)
  , TmRef <$> (symbol "Ref" *> (name))
  , TmLet <$> (symbol "Let" *> (name)) <*> (symbol "=" *> parseTerm)
                           <*> (symbol "in" *> parseTerm)
  ]) <* (optional eof)
\end{lstlisting} \label{code:parser}

Finally two code generators are implemented for the translation into
second and third generation networks.
This translation will be presented along with the implementation
for the two backends.

\subsection{Futhark backend} \label{sec:volr-futhark}
Futhark is a functional data-parallel programming language, that
executes on a number of targets \autocite{Henriksen2017}.
\gls{OpenCL} is one of them, which allows Futhark access to 
hardware acceleration through \gls{GPU}s.

This thesis builds on the work by \textcite{Minh2018}, who 
implemented a functional library for deep learning.
The library models \gls{NN} layers as records with three fields:
a function for forward propagation, a function for backward propagation
and a cache for weights. 
The backward propagation uses gradient descent to find optimal
weight configurations.
Because of its functional nature, layers can simply be joined 
sequentially by function composision.

The library from \cite{Minh2018} has been modified to fit the
additions in the thesis.
In particular, a \textit{replicate} and a \textit{merge} layer has
been added, to account for the parallel operator ($\ominus$).
A connection that allows two parallel networks to connect to other
layers has also been added, although it simply composes tuples of
the layer structure (see the \texttt{neural\_networks.fut} file in
appendix \ref{app:implementation}.

\paragraph{Replicate layer}
In practice this layer connects to two other networks, and densely
replicates the output to each layer.
This happens by storing two separate collections of weights, such
that each forward connection is assigned the correct value.
Backpropagation works by calculating the error correction on the
two sets of weights, and the final error sent to the previous layer
is a simple average of the error for each neuron. 
The algorithm is shown in an shortened form in listing \ref{code:bp-replicate}.
Here the two errors and weights are independently calculated.
The weights are stored in the layer as-is, but the errors are 
averaged before they are output to the previous layer.

\lstset{language=,caption=Part of the forward and backward propagation algorithms for the replicate layer. Abbreviated for clarity.}
\begin{lstlisting}
-- Forward propagation
let forward (act:[]t -> []t)
	    (training:bool)
	    ((t1, t2): weights)
	    (input:input) : (cache, output) =
  let feedForward ((w, b): std_weights t): (tup2d t, arr2d t) = [...]
  let (c1, r1) = feedForward t1
  let (c2, r2) = feedForward t2
  in ((c1, c2), (r1, r2))

-- Backward propagation
let backward (act: []t -> []t)
             (first_layer: bool)
             (apply_grads: apply_grad t)
             ((w1, w2): weights)
             ((c1, c2): cache)
             ((e1, e2): error_in) : b_output =
  let (error1, w1) = backProp w1 c1 e1
  let (error2, w2) = backProp w2 c2 e2
  [...]
  in (average_sum_matrix [error1, error2], (w1, w2))
\end{lstlisting} \label{code:bp-replicate}

This interpretation fits with the original specification, where
where the parallel notation duplicates the 'work' in two separate 
networks.
The whole code for the replicate, and the code for translating the \gls{DSL}
abstractions into Futhark, \index{Futhark} is shown in appendix 
\ref{app:implementation}.

\paragraph{Merge layer}
The merge layer is significantly simpler than the replication layer,
because it simply concatenates the inputs from two parallel layer into one single
layer. 
For that reason it also does not contain any weights.
In the case of backprogapation it simply reroutes the errors back to
the population, from which the neuron originated.
All optimisation logic is thus left for the regular dense or replicated
layers.

\lstset{language=,caption=Functions for forward and backward propagation in the merge layer.}
\begin{lstlisting}
-- Forward propagation
let forward  (_:[]t -> []t)
	     (_:bool)
	     (_: weights)
	     ((i1, i2):input) : (cache, output) =
  ((), map2 concat i1 i2)

-- Backward propagation
let backward (_:[]t -> []t) (l1_sz:i32)
	     (_:bool)
	     (_:apply_grad t)
	     (_:weights)
	     (_:cache)
	     (error_concat:error_in) : b_output =
  (unzip (map (split l1_sz) error_concat), ())
\end{lstlisting}

This interpretation is also aligned with the original specification, 
because it allows the parallel populations to propagate its output
independent of each other.

\begin{comment}
, where the compiler can generate specifications for either Futhark
or third generation neural networks.
In the case of Futhark, a complete text file with correct Futhark syntax
is generated. 
In the other case, an intermediate JSON file is generated for a number of
reasons: one is the heterogeneous nature of the backends.
NEST \index{NEST} is written in C++ \index{C++} and can run on most platforms,
but BrainScaleS requires access to the neuromorphic hardware.
The BrainScaleS chips are expensive to produce and protected by a number of
measures.
One being that certain libraries are inaccessible, excluding the evaluation
of Haskell binaries. 
PyNN is normally configured to work with Python, so converting the 
\end{comment}
% An optimal approach would be to find a tool that leverages the similarities
% of the network types, while integrating with the diverse simulated or emulated
% targets.
% That is, an abstract model of neural networks that can translate into
% heterogeneous back-ends, while retaining a high degree of inter-model validity.

% In practice a network is built by describing a graph
% The nodes in the graph consist of \texttt{populations} of neurons and the edges
% are connection-set matrices to other populations \autocite{Djurfeldt2012}.
% % TODO: Describe CSA
% \texttt{Populations} can consist of any positive number of neurons and is
% required to have at least one connection.
% Connections can be recursive, resulting in a potentially cyclic graph.
% Both the connections and the \texttt{populations} can be annotated with features
% such as connection weight and neuron parameters (see \nameref{appendix:volr}).
% The parameters are treated differently depending on the experiment target (see
% sections \ref{sec:volr-NEST} and \ref{sec:volr-BrainScaleS}).


\subsection{PyNN backend}
The interpretation to PyNN\index{PyNN} is done in two steps: a
conversion from the \gls{DSL} into an \gls{SNN} representation,
and a translation between that representation into backend-specific
Python code.
The steps are decoupled to enforce the same semantics on the code
generation for the neuromorphic and simulation backends.
While PyNN is designed as a simulator-dependent \gls{API} 
it is unlikely that the backends will fully support it in the
immediate future (see section \ref{sec:SNN-simulators}).

\subsubsection{\gls{SNN} translation step}
Before Python code for PyNN can be generated a number of assumptions
has to be met. 
In particular the types of connections (referred to as projections in PyNN)
and neuron models has to be described with a full set of neuron parameters.
The modelling of this happens in yet another Haskell code base.
The neuron parameters for an example LIF\index{LIF} neuron is seen in listing
\ref{lst:lif-cond-exp} with their corresponding default values
below\footnote{
The current neuron models and their default parameters are taken from PyNN's
standard models, available at
\url{http://neuralensemble.org/docs/PyNN/standardmodels.html}.
}.

\lstset{language=,caption={A LIF neuron with exponential decay and
  conductance-based synapses, modelled in Haskell.},label=lst:lif-cond-exp}
\begin{lstlisting}
data NeuronType =
    = IFCondExp {
        _v_rest :: Float, -- ^ resting potential
        _cm :: Float, -- ^ membrane capacitance
        _tau_m :: Float, -- ^ membrane time constant
        _tau_refrac :: Float, -- ^ refractory time
        _tau_syn_E :: Float, -- ^ excitatory synaptic time constant
        _tau_syn_I :: Float, -- ^ inhibitory synaptic time constant
        _e_rev_E :: Float, -- ^ excitatory reversal potential
        _e_rev_I :: Float, -- ^ inhibitory reversal potential
        _v_thresh :: Float, -- ^ spike initiation threshold
        _v_reset :: Float, -- ^ reset value for membrane potential after a spike
        _i_offset :: Float -- ^ offset current
    }

if_cond_exp :: NeuronType
if_cond_exp = IFCondExp {
    _v_rest = -65.0,
    _cm = 1.0,
    _tau_m = 20.0,
    _tau_refrac = 0.0,
    _tau_syn_E = 5.0,
    _tau_syn_I = 5.0,
    _e_rev_E = 0.0,
    _e_rev_I = -70.0,
    _v_thresh = -50.0,
    _v_reset = -65.0,
    _i_offset = 0.0
}
\end{lstlisting}

These neuron models form the basis of populations, which is determined by the
neuron model and the number of neurons in the population.
Populations can be understood as nodes in the network graph, along with
spike sources, which generates stimulus per the input data.
The definition of nodes as either populations or spike sources is shown in
listing \ref{lst:node}.

\lstset{language=,caption=The definition of a node as either a population or a
spike source.,label=lst:node}
\begin{lstlisting}
data Node = Population {
        _numNeurons :: Integer,
        _neuronType :: NeuronType
    }
    | SpikeSourcePoisson {
        _rate :: Float,
        _start :: Integer
    }
\end{lstlisting}

The nodes in the spiking graph is connected through edges shown in 
listing \ref{lst:projection}.

\lstset{language=,caption=The definition of an edge as a projection between
two nodes with a certain effect.,label=lst:projection}
\begin{lstlisting}
data Edge =
      Projection {
          _effect :: ProjectionEffect,
          _input :: Node,
          _output :: Node
\end{lstlisting}

The type of the edges are determined by a projection effect that is
not listed here, but available in appendix \ref{app:implementation}.
This effect simply describes the connectivity between two nodes---which
for now is exclusively all-to-all---and the type of the synapses--which
for now is exclusively constant excitatory weights.

The model here allows us to retain the connection graph from the \gls{DSL}
description.
The main difference being the activation functions which changes with the
neuron types.

\subsubsection{PyNN code generation}
The final step in the translation of the \gls{DSL} to PyNN code maps the
\gls{SNN} model from above to executable Python code.
Similar to the Futhark backend the PyNN API has been augmented to 
incorporate learning, further described in the section ``Learning in PyNN''.
Listing \ref{lst:PyNN-MNIST} shows an example of a non-executable MNIST network.
When activating the input node \texttt{node0} the connections are fed through
the network to the \texttt{node2} output node.
Each projection is initiated with random normal distributed weights, similar
to the Futhark networks.

\lstset{language=,caption={A simple MNIST network in Python from the network in figure
\ref{fig:volr-examples}. The dots illustrate the neuron parameters, given as a
dictionary, as they appear in listing
\ref{lst:lif-cond-exp}.},label=lst:PyNN-MNIST}
\begin{lstlisting}
node0 = LearningNode(Population(100, LIFCondExp(), ...))
node1 = LearningNode(Population(50, LIFCondExp(), ...))
node2 = LearningNode(Population(10, LIFCondExp(), ...))
edge0 = Projection(node0, node1, AllToAllConnector())
edge0.set(weight=numpy.random.normal)
edge1 = Projection(node1, node2, AllToAllConnector())
edge1.set(weight=numpy.random.normal)
\end{lstlisting}

% The advantage of the static experiment representation being, that the experiment
% easily a) transports to the target PyNN scripts without losing any information,
% and b) duplexes between several experiment; the same experiment setup is
% trivial to setup on multiple targets at once.
\subsubsection{Code header for BrainScaleS} \label{ref:Cairo}
%Because of the incompatible backends, each code is injected with a  
%preample to perform the necessary setup steps 
Describe hardware mappings for BrainScaleS

\subsubsection{Learning in PyNN}
\label{ref:PyNN-augmentation}
Describe PyNN learning API
