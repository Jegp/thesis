% !TEX root = ../report.tex

Before diving into the actual experiment built to test the hypothesis, this
chapter discusses and describes the method employed to model spiking
and artificial \gls{NN}s.

Both types of network are architecturally similar, and both are conceived from
the same physiological principles \autocite{dayan2001, russel2007, Nilsson2009, schmidhuber2014}.
The implementations, however, vary greatly.

To ensure internal and external validity in and between the two network types,
it is desirable that the models are as closely related from a theoretical and
practical perspective as possible.
Additionally, to test the hypothesis, it is required that both the artificial
and spiking models can be simulated on regular machine architecture, while
the spiking model requires a neuromorphic hardware platform.

An optimal approach would be to find a tool that leverages the similarities
of the network types, while integrating with the diverse simulated or emulated
targets.
That is, an abstract model of neural networks that can translate into
heterogeneous back-ends, while retaining a high degree of inter-model validity.

A number of general frameworks for artificial neural networks
exist\footnote{
  Among others, see \autocite{ONNX2018}, \autocite{PyTorch2018}, \autocite{TensorFlow2018},
  \autocite{Keras2018} and DyNet \autocite{Neubig2017}.
}, but none of them extend to the spiking domain.
Conversely a number of choices exist for neuromorphic modelling\footnote{
  %TODO: Find sources on internal IBM/Intel stuff
}, but they exclusively evaluate to neuromorphic or spiking neural network
backends \autocite{Jordan2018}.

A domain-specific language (DSL) called Volr was recently presented to
construct reproducible \gls{NN} experiments
\autocite{Pedersen2018:volr}.
The DSL allows the modelling of sufficiently complicated models for
the purpose of this thesis, while providing a set of tools that permits the
model be sent and evaluated on both \gls{ANN} and \gls{SNN} targets.

Some work was required to fully support learning mechanisms on
neuromorphic hardware, and the DSL, as well as the tooling around it, has been
extended for the purpose of this thesis (see appendix \ref{appendix:volr})
The following section describes the grammar and anatomy of Volr in detail.

\section{Volr}
Volr is a declarative language designed to model \gls{NN} that seeks a
trade-off between complete, but verbose, descriptions of small
networks and more general designs of large and complex networks.
By separating the network topologies from the detailed physiological properties
of each neuron or neuron population, the language aims to allow simple
experiments with few, concise declarations, as well as larger and more
complicated experiments while retaining readability.

\subsection{Volr syntax}
This section defines the grammar for Volr step by step, beginning
with the general notion of an \textit{experiment}.
The below definitions omit whitespaces for the sake of brevity\footnote{
Apart from the rules described in section \ref{sec:volr-block}, spaces and
tabs can be inserted between any two grammatical units, whereas newline
or carriage return characters only occur after definitions of blocks,
connections and fields.
}
For completeness the full definition of the language can be found in appendix
\ref{appendix:volr}.

\begin{figure}
  \begin{Verbatim}[commandchars=\\\{\}]
experiment  ::= \textit{stimuli populations responses targets}

connections ::= \textit{connnection} [ \textit{connections} ]
connection  ::= \textbf{from} \textit{name} \textit{cexpr}
cexpr       ::= \textit{cterm}
              | \textit{cexpr} \textit{cop} \textit{cexpr}
cterm       ::= \textbf{all} | \textbf{one} | \textbf{self} | \textbf{random} \textit{number}
cop         ::= \textbf{+} | \textbf{-} | \textbf{*}

stimuli     ::= \textit{stimulus} [ stimuli ]
stimulus    ::= \textbf{stimulus} \textit{name} \textit{fields}

populations ::= \textit{population} [ \textit{populations} ]
population  ::= \textbf{population} \textit{name} \textit{connections} \textit{fields}

responses   ::= \textit{response} [ \textit{responses} ]
response    ::= \textbf{response} \textit{connections} \textit{fields}

targets     ::= \textit{target} [ \textit{targets} ]
target      ::= \textbf{target} \textit{tname}
tname       ::= \textbf{futhark} | \textbf{nest} | \textbf{brainscales}

fields      ::= \textit{field} [ \textit{fields} ]
field       ::= \textit{name} \textbf{:} \textit{value}
value       ::= \textbf{[} \textif{namelist} \textbf{]} | \textit{name} | \textit{number}

namelist    ::= \textit{name} [ \textbf{,} \textit{namelist} ]
name        ::= \textit{letter} \textit{string}
string      ::= \textit{char} [ \textit{char} ]
char        ::= \textit{letter} | \textit{digit}
letter      ::= non-whitespace Unicode character

number      ::= \textit{integer} [ \textbf{.} \textit{nat} ]
integer     ::= [ \textbf{-} ] \textit{nat}
nat         ::= \textit{digit} [ \textit{nat} ]
digit       ::= \textbf{0} | \textbf{1} | \textbf{2} | \textbf{3} | \textbf{4}
              | \textbf{5} | \textbf{6} | \textbf{7} | \textbf{8} | \textbf{9}

  \end{Verbatim}
  \caption{BNF of an experiment in Volr with stimuli, populations, responses and
           targets.}
  \label{fig:volr-bnf}
\end{figure}

Figure \ref{fig:volr-bnf} shows the constituent parts of an experiment.
Four sub-com\-po\-nents are required:
  A number of stimuli, neural populations, responses and experiment targets.

\subsubsection{Block grammar} \label{sec:volr-block}
The sub-components are constructed using the same declarative \textit{block}
structure shown in figure \ref{fig:volr-ebnf-block}.
The block defines its \textit{type} (\texttt{stimulus}, \texttt{population},
\texttt{response} or \texttt{target}), an optional \textit{name} and lastly
some block content.

The content varies for the different block types, but is restricted to contain
a number of either key-value pairs (\texttt{field}s) or relations to other
blocks (\texttt{connection}s).
Fields are interpreted by the respective targets (see section
\ref{sec:volr-targets}) and connections are only allowed for
\texttt{population}s and \texttt{responses}.

\subsubsection{Connection-set algebra grammar} \label{sec:volr-csa}
Connections between blocks are implemented as a subset of the \gls{CSA}
introduced in section \ref{sec:CSA} \autocite{Djurfeldt2012}.
Most notably, the notion of \textit{blocks} and \textit{geometric distance} have
been omitted\footnote{Neither blocks nor distances are required for the
  experiment in the thesis. It is, however, a necessary element to conduct
  further experiments, since biological \gls{SNN} are highly influenced by
  spatial arrangements \autocite{dayan2001}.
}.

The grammar is presented in figure \ref{fig:volr-ebnf-csa}, and aims to provide
a flexible way to describe complex connectivity patterns in text.
The present grammar can be viewed as a basic algebraic tool for set operations.
Describing full connectivity between two populations, such that all neurons
in the first population is connected with all neurons in the second population,
can simply be expressed as `\texttt{all}'.
Connecting one neuron to one other neuron between two populations are described
as `\texttt{one}', while random connectivity are described with a probability
of, say 0.5: `\texttt{random 0.5}'\footnote{
  This expresses a Bernouilli trial with the given probability
  \autocite{Djurfeldt2012}.
}
As a final example, every neuron in a population connected to every neuron in
that same population, with the exception of identical neurons, can be described
as `\texttt{all - self}'.
%
% \begin{figure}
%   \begin{center}
%     \begin{minipage}{0.8\linewidth}
%       \begin{grammar}
%         <connection> ::= 'from' , <block-name> , { <csa-expr> } ;
%
%         <csa-expr> ::= <csa-term> | <csa-expr> , <csa-operator> , <csa-expr> ;
%
%         <csa-term> ::= 'all' | 'one' | 'self' | 'random' , <number> ;
%
%         <csa-operator> ::= '+' | '-' | '*' ;
%       \end{grammar}
%     \end{minipage}
%   \end{center}
%   \caption{BNF of the connection grammar, describing relations between
%     blocks through \gls{CSA}.}
%   \label{fig:volr-bnf-csa}
% \end{figure}

\subsubsection{Experiment stimuli}
The stimuli describes the ``input'' of the model.
Such input is defined either as an array of elements directly in the DSL
or as a reference to a file.

\subsubsection{Experiment populations}
The populations describe the topology of the neural network itself.
As with the stimuli, the populations are built around a block structure that
contains a number of sub-expressions.

The \texttt{connection} defines the source stimulus for the population,
i.e. the population \textit{from} which action-potentials will be forwarded.
A population can receive stimulus from more than one source.
The connections are modelled as per the \gls{CSA} described in section
\ref{sec:volr-csa}.

% TODO: Describe and invent archetypes... or not?

\subsubsection{Experiment responses}
The responses are the ``output'' of the model to be recorded, and can be
considered as the outcome of the network for training purposes.
The response block only contains an optional specification of a location for
the experiment output data.

\subsubsection{Experiment targets}
The final element in a Volr experiment is its targets.
A target describe a destination environments on which to run the experiment.
These are described in detail in section \ref{sec:volr-targets}, and are
referenced in the grammar as simple strings.

\pagebreak
\subsection{Volr model validity}
Not all experiments are
... multipass compiler
... checks for valid/invalid experiments

\subsection{Volr semantics}
In practice a network is built by describing a graph.
The nodes in the graph consist of \texttt{populations} of neurons and the edges
are connection-set matrices to other populations \autocite{Djurfeldt2012}.
% TODO: Describe CSA
\texttt{Populations} can consist of any positive number of neurons and is
required to have at least one connection.
Connections can be recursive, resulting in a potentially cyclic graph.
Both the connections and the \texttt{populations} can be annotated with features
such as connection weight and neuron parameters (see \nameref{appendix:volr}).
The parameters are treated differently depending on the experiment target (see
sections \ref{sec:volr-NEST} and \ref{sec:volr-BrainScaleS}).

\section{Neural network simulation targets in Volr} \label{sec:volr-targets}

% TODO :Write how fields are interpreted
% TODO: Write how input is interpreted

Volr exploits the structural similarities between \gls{ANN} and \gls{SNN} to
translate the model to both spiking and artificial network platforms (back-ends).

In the remainder of the chapter the three emulation back-ends, shown in figure
\ref{fig:volr}, are described:
a machine learning target for \gls{ANN}s and a neuron simulation target, as well
as a neuromorphic hardware target, for \gls{SNN}s.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{images/volr-architecture.png}
  \caption{The translation from the Volr DSL to \gls{ANN} simulations in OpenCL via
    \gls{Futhark} and to \gls{SNN} simulations on \gls{NEST} and \gls{BrainScaleS}
    via the \gls{Myelin} middleware.
  }
  \label{fig:volr}
\end{figure}

\subsection{Translation to Futhark} \label{sec:volr-futhark}
Futhark is a functional data-parallel programming language \autocite{Henriksen2017}.
It offers a number of compilation targets such as \gls{OpenCL}, which is
particularly interesting for this thesis because of its capacity for hardware
acceleration.

The practical translation from the Volr model to Futhark is built on recurrent
\gls{ANN} with stochastic gradient descent backpropagation
\autocite{russel2007, schmidhuber2014}.
Each neuron population is considered as a single layer, whose connections are
determined by a connection matrix.

% Deal with recurrent connections
% Describe how this relates to layers

... To be continued ...

\subsection{Spiking neural network simulations via PyNN} \label{sec:volr-pynn}
The Python neural network simulation interface PyNN is designed as a
"simulator-independent language for building neuronal network models"
\autocite{PyNN2018}.
It aims to reduce the problem of diverse, and occasionally unique, descriptions
of neural network experiments for different simulation back-ends \autocite{Davison2009}.
PyNN has been adapted by a number of simulators, including the NEST simulation
platform and the neuromorphic BrainScaleS wafer system
\autocite{Davison2009, Helias2012, Schmitt2017}.

There are still simulator-dependent configurations that seems unlikely to be
adopted into PyNN in the immediate future\footnote{
  Particularly hardware mapping configurations are hard to abstract in a general
  interface.
}.
For that reason Volr provides simulation-specific PyNN scripts that can
interpret the model in the context of each simulation target.
A middleware, dubbed \gls{Myelin}, was invented to translate the \gls{NN} model
into a static intermediate representation in JSON.
The JSON standard was chosen for the task because of its concise syntax while
still retaining human readability.

The advantage of the static experiment representation being, that the experiment
easily a) transports to the target PyNN scripts without losing any information,
and b) duplexes between several experiment; the same experiment setup is
trivial to setup on multiple targets at once.

The correct execution of the experiments relies on the PyNN scripts to exploit
the simulator to represent the Volr model as accurately as possible.
Fortunately PyNN is designed to cover exactly such a use case, so properties
related to the \gls{NN} models itself (such as network topology and population
attributes) were faithfully reproduced across the simulators.
However, the simulators deviate in a number of ways that are relevant to
mention.
The following two sections explains the steps necessary to achieve accurate
experiment environments in \gls{NEST} and \gls{BrainScaleS}.

\subsubsection{Translation to PyNN} \label{sec:volr-translation}

\subsubsection{Translation to NEST} \label{sec:volr-NEST}
... To be continued ...
\subsubsection{Translation to BrainScaleS} \label{sec:volr-BrainScaleS}
... To be continued ...
