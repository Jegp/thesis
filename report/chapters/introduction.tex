\documentclass[report.tex]{subfiles}
\begin{document}

The field of machine learning is rapidly evolving, and have in
some recognition tasks surpassed human-level precision
\autocite{Schmidhuber2014}.
This acceleration is propelled by the advanced in artificial neural
networks \autocite{Rumelhart1988, Schmidhuber2014, Nilsson2009}, or
so-called \textit{second} generation networks \cite{Maass1997}.
The architecture behind these networks developed from the neuroscientific
idea of a neuron \autocite{Nilsson2009, Russel2007}.
Contemporary cognitive neuroscience however, tells us that learning in
the biological brain occurs in spiking neural networks, that emits
signals in \textit{spikes} over time \cite{Dayan2001, Eliasmith2004}.
This \textit{third} generation of neural networks permits the encoding
of information over time, but at the cost of stability and 
noise \autocite{Maass1997}.

Because of their biological similarities, this third generation of
networks are of great interest to neuroscientists
\autocite{Dayan2001,Bruderle2011,Eliasmith2015}.
%Currently the networks are explored either as pure simulations
%\autocite{Davison2009} or as analogue circuits
%that resemble the physical structure of neural networks
%\autocite{Walter2015, Schmitt2017}, 
Compared to experimental studies on living neural substrate,
they provide a cheap and efficient way to experiment with cognitive 
models \autocite{Eliasmith2015}.
However, the programming knowledge required to construct and conduct
such experiments are beyond the reach for most neuroscientists.
% TODO: Cite

Parallel to the advancements in computer science and machine learning,
efforts have been made to map the functional connectivity of the mammalian
brain (the connectome) to the processes of cognition \autocite{Mogensen2018}.
The model for the \textit{reorganisation of elementary functions} (REF)
is presented by \textcite{Mogensen2017} as an initial step towards
relating conscious states to neural matter.

% It is unclear whether the advances in layered neural networks can 
% reproduce the complicated mechanisms for learning and cognition,
% brought about in neurobiological systems \autocite{Eliasmith2015, Pearl2018}.

% \textcite{Eliasmith2015} have implemented von Neumann style computation 
% in neurobiological systems, and a number of studies
% have shown that spiking neural networks are capable of solving a wide
% range of learning tasks \autocite{Pfeil2013, Tavanei2015, Walter2015},
% proving that their .
\vskip10pt
Realising the importance of accessible, realistic \textit{third} 
generation neural simulations for the cognitive sciences, this thesis
sets out to build a domain-specific language.
The language prototypes the translation of cognitive concepts 
such as learning into neural network topologies, that, in turn,
translates to either spiking or layered neural networks.

Guided by the REF model, a neural network capable of solving a simple
maze-task will be modelled as a use-case.
The network will be evaluated in both simulated and analogue
third generation neural network environments as well as a single second
generation neural network. 

% explore the behaviour of neural network models in 
%spiking and layered architecture.
%Based on a Krechevky maze-learning task and the theory of
%the reorganisation of elementary Funtions (REF), this thesis sets out
%to explore how a more accessible domain specific language (Volr)
%can help to build and evaluate experiments in neuromorphic hardware.
The goal is to lay the foundation for a robust experimental framework
for third generation neural networks, accessible to cognitive and 
computer scientists alike.

\section{Hypothesis}
This thesis examines the hypothesis that *the model for the Reorganisation
of Elementary Functions can be implemented using spiking neural networks*.

The hypothesis drives two outcomes: a spiking neural network
representation of the REF model and a Krechevky maze experiment.

\subsection{Scope}
The hypothesis will be evaluated based on two criteria: the similarity of
the spiking REF model to contemporary neurocognitive theories on the
mammalian brain as well as its capacity to learn a given problem.

The thesis will focus on how well the the model describes contemporary
neurocognitive theories of learning, keeping the limits of the
experimental platforms in mind.
Because the REF model has not been mapped to its physiological
properties, it is outside the scope of this thesis to provide an exact
comparison to the biological properties.

To provide a context for the learning capacity of the spiking neural networks, they will be compared to a regular non-spiking neural network.

\section{Experimental setup}
Three models will be built: a spiking neural network simulation via NEST [11], a spiking neural network emulation via BrainScaleS [6] and a layered non-spiking neural network written in Futhark [12].
All three models will be trained to perform the same Krechevky maze task.

Since the neuromorphic hardware is significantly more performant than the software simulation, it is desirable to run it on chip.
It is however, important to retain the simulation as a baseline, because of unexpected analogue effects of the neuromorphic hardware.

The non-spiking neural network will be written in Futhark and executed using OpenCL.
\section{Cognitive neuroscience}
\section{Neuromorphic hardware}
% Motivation: platform for simulation
\section{Hypothesis} \label{sec:hypothesis}
This thesis examines the hypothesis that *the model for the Reorganisation of Elementary Functions can be implemented using spiking neural networks*.
\subsection{Evaluation criteria} \label{sec:hypothesis-criteria}


\end{document}