\begin{document}

The field of \gls{ml} is evolving rapidly, and has in
some recognition tasks surpassed human-level precision
\autocite{Schmidhuber2014}.
This acceleration is propelled by the advances in \glspl{ANN}, 
which recently defeated a human in the
advanced real-time strategy game Starcraft II \cite{DeepMind2019}
\autocite{Schmidhuber2014, Nilsson2009, Russel2007}.
\citeauthor{Maass1997} dubs \glspl{ANN} second generation
\glspl{NN}, because they supercede the first generation networks
based on the perceptron.
He believes they themselves will be superceded by a third generation of neuron
models that closely resemble biology.
Unlike neurons in \glspl{ANN} that follow well-behaved continuous
functions, biological neurons communicate by spikes of
electricity over time
\cite{Maass1997}.

Because of their biological similarities, third generation 
\glspl{NN} are of great interest to (cognitive) neuroscientists
\autocite{Dayan2001,Bruderle2011,Eliasmith2015}.
Compared to experimental studies involving living neural substrate,
it is significantly cheaper and faster to build neural 
models either as pure simulations
\autocite{Davison2009, Eliasmith2015} or as analogue circuits
that resemble the physical structure of neural networks
\autocite{Walter2015, Schmitt2017}.
Furthermore, researchers have complete control over virtual models
to pause, lesion, or even disassemble at will.

Neuromorphic computation is a paradigm that aims to exploit this new
generation of network models, by constructing circuits that encode information
in spikes over time instead of digital signals \cite{Eliasmith2015, Albada2018}.
The neuromorphic neuron model can be built in
silicon and have shown to accelerate the performance of \glspl{NN}
by a factor of up to $10^5$ \cite{Albada2018, Schmitt2017}.

A challenge for third generation networks is the relatively
poor understanding of learning processes within spiking neurons \autocite{Tavanei2015,
Walter2015}.
This topic is subject to intense research, and there is a growing body of work
that attempts to validate the theories through simulated experiments 
\cite{Indiveri2015, Tavanaei2019}.
Within the field of \gls{ml} learning is a well-researched topic, and in the
absence of clear neurophysiological learning models, it is a common approach to 
explore learning algorithms from \gls{ml} in the simulated neural systems
\cite{Lee2016,Schmitt2017,Walter2015,Diehl2015,Rueckauer2017}.
The landscape for neural simulations are, however, heterogeneous and the
simulated models typically imply a number of assumptions (such as neuron
parameters and model topology), that renders the experiments near-incommensurable
\cite{Albada2018, Lee2016, Rueckauer2017}.
The outcome is that the experimental findings are difficult to validate
and re-integrate with theoretical models \autocite{Rueckauer2017, Albada2018,
Bruderle2011}.

This thesis sets out to explore \glspl{SNN} and their potential for the 
field of \gls{ml}, focusing on two major challenges for the
third generation models: homogeneous modelling and learning.
\\[0.1cm]

The thesis is built around the hypothetico-deductive model, in which 
falsifiable hypotheses are formulated, tested and evaluated.
The following two sections will present the hypotheses, the methods for
evaluating the hypotheses and finally the thesis structure.

\section{Hypotheses} \label{sec:hypotheses}
This thesis examines two hypotheses:

\begin{enumerate}
  \item The Volr DSL can translate into spiking and non-spiking neural networks such that the network topologies are retained.
  \item Using training it is possible for spiking and non-spiking models to solve an MNIST recognition task.
\end{enumerate}

The hypotheses are driven by two inquiries around modelling and learning.

\paragraph{Hypothesis 1: DSL modelling}
The first hypothesis tests that the neural networks generated by the DSL are
modelled correctly, and translated---without significant
deviations---to second and third generation neural networks.
Consistent translations are important to ensure correct and reproducible experiments,
but are also vital to further the understanding of spiking neural networks:
correct rendition bridges the semantics of artificial and spiking neural networks.
This indirectly allows users of the DSL to draw on the vast literature of
second generation \gls{NN}s.

To test this hypothesis, it is necessary to derive a common abstraction for
\glspl{NN} of both second and third generation, and to provide proof that the
abstraction can be converted into functioning spiking and non-spiking neural
models.

For that purpose a compiler will be built that translates neural models
into two target paradigms:
  1) a second generation \gls{NN} based on the data-parallel language Futhark, and
  2) a third generation \gls{NN} based on the neural simulator Neural Simulation
  Toolkit (NEST).
  \index{Futhark}\index{NEST}
  The models are expected to resemble each other in topology. 
That means that they consist of identical collections of nodes, edges
and connectivity descriptions.

\paragraph{Hypothesis 2: Learning}
Second generation \gls{NN}s are generally capable of learning pattern-recognition
problems \cite{Russel2007}.
It is known that learning also occurs within neural systems, so a
similar behaviour is expected in third generation \gls{NN}s.
The second hypothesis verifies that this property exists in both spiking and
non-spiking neural networks.
If it does not, the \gls{DSL} has failed to capture
the learning capacities of the second generation networks.
Additionally, the hypothesis provides a mean of comparison between the two paradigms.

To test the hypothesis, it is necessary to prove that learning occurs
in both paradigms.
Three experiments were designed to test this: two trivial
logical gates, NAND and XOR, and a recognition task of handwritten digits
(MNIST).
The experiments will be executed in both second and third generation
environments.
Afterwards, the results will be compared based on the ability to predict the correct outcome,
as well as the speed and quality of learning.

\section{Thesis structure}
The next chapter builds the theoretical basis of the thesis, by defining and
establishing relevant theoretical concepts.
Then the design and implementation of the \gls{DSL} will be elaborated on.
Chapter \ref{sec:experiment} presents the experimental setup that aims to
test the hypotheses, and the following chapter \ref{sec:results} presents and
analyses the results of the experiments.
The last chapter discusses the findings of the thesis, concludes on the
hypotheses, and finally reflects on the approach of the thesis before proposing
areas of future work.

\end{document}
