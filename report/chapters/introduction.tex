\documentclass[report.tex]{subfiles}
\begin{document}

The field of machine learning is rapidly evolving, and have in
some recognition tasks surpassed human-level precision
\autocite{Schmidhuber2014}.
This acceleration is propelled by the advanced in artificial neural
networks \autocite{Rumelhart1988, Schmidhuber2014, Nilsson2009}, or
so-called \textit{second} generation networks \cite{Maass1997}.
The architecture behind these networks developed from the neuroscientific
idea of a neuron \autocite{Nilsson2009, Russel2007}.
Contemporary cognitive neuroscience however, tells us that learning in
the biological brain occurs in spiking neural networks, where neurons
communicates in \textit{spikes} over time \cite{Dayan2001, Eliasmith2004}.
This \textit{third} generation of neural networks permits the encoding
of information over time, but at the cost of stability and 
noise \autocite{Maass1997}.

Because of their biological similarities, this third generation of
networks are of great interest to neuroscientists
\autocite{Dayan2001,Bruderle2011,Eliasmith2015}.
Compared to experimental studies involving living neural substrate,
it is significantly cheaper and faster to build cognitive 
models either as pure simulations
\autocite{Davison2009, Eliasmith2015} or as analogue circuits
that resemble the physical structure of neural networks
\autocite{Walter2015, Schmitt2017}.
Furthermore, virtual models are under the complete control
of the researcher and can be paused, lesioned or even
disassembled.

This is particularly interesting for the efforts done in the
mapping of functional connectivity (the human connectome) to the
processes of cognition \autocite{Eliasmith2015, Mogensen2011}. 
\textcite{Mogensen2011} presented a model for the 
\textit{reorganisation of elementary functions} (REF), based on
numerous clinical trials and lesion studies.
However, the theory is challenging to verify in vivo because of
the high level of interconnectivity in the mammalian brain
\autocite{Hohwy2009, Mogensen2017}.

The present study attempts to extend the innovations within
third generation neural networks to the field of cognitive 
neuroscience. 
By constructing a means to translate higher-order neural
concepts into neural network simulations, the aim is to allow
the .
To verify this claim, a small neural model based on the the
\gls{REF} theory is built and asked to solve a small maze task.
The model will be implemented in spiking and non-spiking
substrates to test its relevance to the domain of neuroscience.

\section{Hypothesis} \label{ref:hypothesis}
The thesis examines the hypothesis that 
\textit{the theory for the reorganisation of elementary functions
(\gls{REF}) can be implemented in third generation \gls{NN}s using
the Volr DSL}.

Before the hypothesis can be formally tested, it is necessary to
present a method to translate the constituent parts of the \gls{REF} theory
into neural network components.
This is the role of the \gls{DSL}, which will be elaborated
in the first part of the thesis.
The following second part sets out to define a neural network
model built with the above \gls{DSL} as well as a small T-maze experiment. 
The model acts as a translation of the concepts of the \gls{REF} model,
to the delimited maze task.
The third part will translate the neural network model 
into one second generation based on Futhark, and two third
generation implementations based on the neural simulator \index{NEST} and 
the neuromorphic hardware platform \index{BrainScaleS} respectively.
All three implementations will be benchmarked based on their ability 
to solve the maze task.
The results of the benchmark will be presented and discussed in the the
fourth and final part of the thesis.
This part will also relate the experimental results to the
hypothesis and reflect on their implications as well as future work.

Specifically the hypothesis will be tested based on
a) the ability of the model to learn the maze problem as well as 
b) the proximity of the neural model to the components of the
\gls{REF} theory.

Regarding a) it is entirely possible that the second generation networks prove
to be better at adapting to the domain compared to the third generation
implementations.
If that is the case, the \gls{DSL} has failed to translate
the learning capacities of the second generation networks, disproving the
hypothesis.

Another way to falsify the hypothesis is a significant
dissimilarity\footnote{See \ref{sec:ref} for a definition of
\enquote{dissimilar}.} between the neural network implementations 
and the abstract \gls{REF} model. 
In that case the \gls{DSL} is unable to accurately translate the
necessary components of the theoretical framework.

\subsection{Scope} \label{sec:scope}
While this thesis will employ similar mechanisms for learning, 
it is outside the scope of this thesis to provide an exact 1:1 mapping
between the implementations.
This will naturally have consequences for the subsequent analysis of the
later experimental results (see section \ref{sec:discussion}).

The \gls{REF} model is a general and abstract description of cognitive
processes, and no attempts have as of yet been made to map it to
computational models.
This first attempt is only made feasible by focusing on select parts
of the \gls{REF} model. 
Specifically higher-order cognitive functions as well as detailed 
biological similarities requires much more extensive studies, and are
considered to be outside the scope of this thesis.

\end{document}