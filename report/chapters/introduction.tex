\documentclass[report.tex]{subfiles}
\begin{document}

The field of \gls{ml} is rapidly evolving, and have in
some recognition tasks surpassed human-level precision
\autocite{Schmidhuber2014}.
This acceleration is propelled by the advances in \gls{ANN}
\autocite{Rumelhart1988, Schmidhuber2014, Nilsson2009}, or
so-called \textit{second} generation \gls{ANN} \cite{Maass1997}\footnote{Not to be 
understood as more \textit{advanced} than previous generations, see page \pageref{sec:glossary}.}.
While the first generation networks builds on the \textit{McCulluch-Pitts}
neuron model that only produces binary outputs (0 or 1), the second
generation uses a sequential layered approach with a continuous set of 
possible outcomes \cite{Maass1997, Russel2007}.
Contemporary cognitive neuroscience however, tells us that the computation
in the biological brain uses temporally separated \textit{spikes}, 
dispatched within a highly connected and recurrent architecture 
\cite{Dayan2001, Eliasmith2004}.
This \textit{third} generation of \gls{NN} permits the encoding
of information over time, but at the cost of stability and 
noise \autocite{Maass1997}.

Because of their biological similarities, third generation of
networks are of great interest to neuroscientists
\autocite{Dayan2001,Bruderle2011,Eliasmith2015}.
Compared to experimental studies involving living neural substrate,
it is significantly cheaper and faster to build \index{cognitive} cognitive
models either as pure simulations
\autocite{Davison2009, Eliasmith2015} or as analogue circuits
that resemble the physical structure of neural networks
\autocite{Walter2015, Schmitt2017}.
Furthermore, researchers have complete control over virtual models
to pause, lesion or even disassembled them at will.

This is particularly interesting for the efforts done in the
mapping of functional connectivity (the human connectome) to the
processes of cognition \autocite{Eliasmith2015, Mogensen2011}. 
\textcite{Mogensen2011} presented a model for the 
\textit{reorganisation of elementary functions} (REF), based on
numerous clinical trials and lesion studies.
However, the theory is challenging to verify in vivo because of
the high level of interconnectivity in the mammalian brain
\autocite{Hohwy2009, Mogensen2017}.

The present study attempts to extend the innovations within
second and third generation \gls{NN} to the field of cognitive \index{cognitive}
neuroscience. 
By constructing a means to translate higher-order neural
concepts into neural network simulations, the aim is to allow
the construction and verification of clinically relevant neural models.
To test this claim, a small neural model based on the
\gls{REF} theory is built and applied to a small maze task.
The model will undergo several smaller permutations to examine
how it aligns with theoretical expectations.
The model will be implemented in spiking and non-spiking
substrates to test its relevance to the domain of neuroscience.

\section{Hypothesis} \label{ref:hypothesis}
The hypothesis is that 
\textit{the theory for the reorganisation of elementary functions
(\gls{REF}) can be implemented in third generation \gls{NN}s using
the Volr DSL}.

Before the hypothesis can be formally tested, it is necessary to
present a method to translate the constituent parts of the \gls{REF} theory
into \gls{NN} components.
Such a translation is presented in the form of the \gls{DSL} \textit{Volr} \index{Volr}
in the first chapter.
The following second chapter defines a \gls{NN}
model built with the above \gls{DSL} as well as a small T-maze experiment. 
The model acts as a translation of the concepts of the \gls{REF} model,
to the delimited maze task.
The third part will translate the neural network model 
into one second generation based on Futhark \index{Futhark} and two third
generation implementations based on the neural simulator \gls{NEST} and 
the neuromorphic hardware platform BrainScaleS \index{BrainScaleS} respectively.
All three implementations will be benchmarked based on their ability 
to solve the maze task, while the two third generation models will be measured
on how well they capture the concepts of the cognitive \index{cognitive}
\gls{REF} model.
The results of the benchmark will be presented and discussed in the the
fourth and final part of the thesis.
This part will also relate the experimental results to the
hypothesis and reflect on their implications as well as future work.

Specifically the hypothesis will be tested based on
a) the ability of the model to learn the maze problem as well as 
b) the proximity of the neural model to the components of the
\gls{REF} theory.

Regarding a) it is entirely possible that the second generation networks prove
to be better at adapting to the domain compared to the third generation
implementations.
If that is the case, the \gls{DSL} has failed to translate
the learning capacities of the second generation networks, disproving the
hypothesis.

Another way to falsify the hypothesis is a significant
dissimilarity\footnote{See \ref{sec:ref} for a definition of
\enquote{dissimilar}.} between the neural network implementations 
and the abstract \gls{REF} model. 
In that case the \gls{DSL} is unable to accurately translate the
necessary components of the theoretical framework.

\subsection{Scope} \label{sec:scope}
While this thesis will employ similar mechanisms for learning, 
it is outside the scope of this thesis to provide an exact 1:1 mapping
between the implementations.
This will naturally have consequences for the subsequent analysis of the
later experimental results (see section \ref{sec:discussion}).

The \gls{REF} model is a general and abstract description of cognitive
processes, and no attempts have as of yet been made to map it to
computational models.
This first attempt is only made feasible by focusing on select parts
of the \gls{REF} model. 
Specifically higher-order cognitive functions as well as detailed 
biological similarities requires much more extensive studies, and are
considered to be outside the scope of this thesis.

\end{document}
