\begin{document}

The field of \gls{ml} is evolving rapidly, and have in
some recognition tasks surpassed human-level precision
\autocite{Schmidhuber2014}.
This acceleration is propelled by the advances in \gls{ANN}s, 
which recently managed to defeat a human in the
advanced real-time strategy game Starcraft II \cite{DeepMind2019})
\autocite{Schmidhuber2014, Nilsson2009, Russel2007}.
\citeauthor{Maass1997} calls \glspl{ANN} for second generation
\glspl{NN}, because they supercede the first generation networks
(based on the perceptron), and because he believes they themselves will be
superceded by a third generation of neuron models.
These neurons resembles the biological neurons in the brain that communicates
not through continuous values in a matrix, but by neuron spikes over time
\cite{Maass1997}.

Because of their biological similarities, third generation 
\glspl{NN} are of great interest to (cognitive) neuroscientists
\autocite{Dayan2001,Bruderle2011,Eliasmith2015}.
Compared to experimental studies involving living neural substrate,
it is significantly cheaper and faster to build neural 
models either as pure simulations
\autocite{Davison2009, Eliasmith2015} or as analogue circuits
that resemble the physical structure of neural networks
\autocite{Walter2015, Schmitt2017}.
Furthermore, researchers have complete control over virtual models
to pause, lesion, or even disassembled at will.

Neuromorphic computation is a paradigm that aims to exploit this new
generation of network models, by constructing circuits that encode information
in spikes over time instead of digital signals \cite{Eliasmith2015, Albada2018}.
The advantage being that the neuromorphic neuron model can be built in
silicon and accelerate the performance of \glspl{NN} by a factor of up to
$10^5$ \cite{Albada2018, Schmitt2017}.

A challenge for third generation networks is the relatively
poor understanding of learning mechanisms \autocite{Tavanei2015,
Walter2015}.
Several different techniques have been applied, but they are
difficult to validate because of the heterogeneous environments and
the incommensurability of the models \autocite{Bruderle2011, Albada2018}.

This thesis sets out to explore \glspl{SNN} and their potential for the 
field of \gls{ml}, focusing on two major challenges for the
third generation models: homogeneous modelling and learning.
\\[0.1cm]

The thesis is built around the hypothetico-deductive model, in which 
falsifiable hypotheses are formulated, tested and evaluated.
The remainder of the introduction will present the hypotheses, the methods for
evaluating the hypotheses and finally the thesis structure.

\section{Hypotheses} \label{sec:hypotheses}
This thesis examines two hypotheses:

\begin{enumerate}
  \item The Volr DSL can translate into spiking and non-spiking neural networks such that the network topologies are retained.
  \item Using training it is possible for spiking and non-spiking models to solve an MNIST recognition task.
\end{enumerate}

The hypotheses are driven by two inquiries around modelling and learning.

\paragraph{Hypothesis 1: DSL modelling}
The first hypothesis tests that the neural networks generated by the DSL is
modelled correctly, and that they translate---without significant
deviations---to second and third generation neural networks.
Consistent translations are important to ensure correct and reproducible experiments,
but are also vital to further the understanding of spiking neural networks:
correct rendition bridges the semantics of artificial and spiking neural networks.
In turn, this allows users of the DSL to draw on the vast literature of
second generation \gls{NN}s.

To test this hypothesis it is necessary to derive a common abstraction for
\glspl{NN} of both second and third generation, and to provide proof that the
abstraction can be converted into functioning spiking and non-spiking neural
models.

To test the hypothesis a compiler will be built that translated neural models
into two target paradigms:
  1) a second generation \gls{NN} based on the data-parallel language Futhark, and
  2) a third generation \gls{NN} based on the neural simulator Neural Simulation
  Toolkit (NEST).
  \index{Futhark}\index{NEST}
The model are expected to resemble each other in topology, meaning that they
consist of identical collections of nodes, edges and connectivity descriptions.

\paragraph{Hypothesis 2: Learning}
It is known that second generation \gls{NN}s are capable of learning
pattern-recognition problems \cite{Russel2007}.
This adaptation is a fundamental property of neural systems, so a
similar behaviour is expected in third generation \gls{NN}s.
The second hypothesis verifies that this property exists in both spiking and
non-spiking neural networks.
If it does not, the \gls{DSL} has failed to capture
the learning capacities of the second generation networks.
Additionally, it provides a mean of comparison between the two paradigms.

To test whether this is the case, it is necessary to prove that learning occurs
in both paradigms.
Three experiments have been constructed to test this: two trivial
logical gates, NAND and XOR, and a recognition task of handwritten digits
(MNIST).
The experiments will be executed in both second and third generation
environments and compared based on their ability to predict the correct outcome,
and the speed and quality of their learning.
While the hypothesis is strictly only concerned with the MNIST dataset, the NAND
and XOR provides a frame of reference for the experiments.

\section{Thesis structure}
The thesis is structured into five parts:
a theoretical part that defines and establishes theoretical concepts,
a section on the design and implementation of the \gls{DSL}, a section on the
experimental setup, a section on the thesis results and finally a discussion
that concludes on the result and considers future work.

\end{document}
