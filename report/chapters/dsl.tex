\documentclass[report.tex]{subfiles}
\begin{document}

This chapter presents the \gls{DSL} Volr \index{Volr}.

Before presenting and specifying the language itself, 
it is beneficial to examine existing work within the
programming and simulation of second and third
generation \gls{NN}s.
Following the existing work, a detailed list of requirements
are provided to accurately scope the \gls{DSL}.
Finally, the implementation of the \gls{DSL} is presented and
accompanied by code and test examples.

\section{Similar work}
A vast amount of work has been put into the development of software for simulating
neural networks.
This section covers recent work within the simulation of second and third
generation \glspl{NN}, and extracts relevant findings for use in the
requirements section \ref{sec:requirements}.

It is likely that the following list is incomplete due to the fragmented and 
fast-paced nature of the field.
To encompass as many relevant findings as humanly possible, the subsequent
references are based on a number of source, primarily based on books
(\cite{Bishop2006, Russel2007, Eliasmith2015, Lin2018, Nilsson2009, Pearl1988, Rojas1996, Rumelhart1988}), review papers 
(\cite{Schmidhuber2014, Blundell2018, Markram2013, Walter2015, Hunsberger2015}),
and, of course, articles, which will be referenced below.
%TODO:  https://github.com/Shikhargupta/Spiking-Neural-Network

\subsection{Second generation}
The perhaps most notable product for this type of networks is the Tensorflow \index{Tensorflow}
framework \cite{Abadi2016}.
Tensorflow is an \gls{API} for the description and execution of directed graph 
structures,
that connects varying activation functions and learning mechanisms through the common abstraction
of tensors \cite{Abadi2015}.
Tensorflow is the result of a large collaboration of multiple companies and organisations, who have
developed a comprehensive library of both code as well as infrastructure, and they
provide extensive support for hardware acceleration \cite{Abadi2015}.

The primary advantage of Tensorflow \index{Tensorflow} arrives from 
its foundation in tensors as a general abstraction, that
can be applied to a wide array of problems \cite{Abadi2016}.
Other frameworks have adapted a similar approach, such as PyTorch \cite{PyTorch2018}, 
scikit-learn \cite{Sklearn2018}, Microsoft Cognitive Toolkit (CNTK) \cite{CNTK2018},
Caffe \cite{Caffe2018} and Theano \cite{Theano2018}.

Lasagne and Keras are built on top of Theano and Tensorflow respectively,
and works similarly well higher-level abstractions \cite{Lasagne2018, Keras2018}.
They both provide imperative \gls{API}s for constructing models in steps, while
including useful utilities for preprocessing of data.

Where scikit-learn, CNTK, Tensorflow and Theano targets \glspl{NN} in general,
PyTorch and Caffe are frameworks that specifically target \gls{DL} 
\cite{PyTorch2018, Caffe2018}.
All of them, however, rely on second generation \gls{NN} architecture in a general sense.

To provide a comparison between them, a number of examples are provided below.
The Listing \ref{code:caffe} shows a network with a single fully connected
layer in Caffe, built to recognise handwritten digits from the 
popular MNIST dataset \cite{LeCun1998}.
Caffe is verbose compared to the full network definitions in PyTorch 
(Listing \ref{code:pytorch}), Keras (Listing \ref{code:keras}) and
Lasagne (Listing \ref{code:lasagne}), but provides additional 
configuration options for setting of weights and biases in 
individual layers.

\lstset{caption={A network layer for the MNIST task in Caffe.}}
\begin{lstlisting}
layer {
  name: "ip1"
  type: "InnerProduct"
  param { lr_mult: 1 }
  param { lr_mult: 2 }
  inner_product_param {
    num_output: 500
    weight_filler { type: "xavier" }
    bias_filler { type: "constant" }
  }
  bottom: "pool2"
  top: "ip1"
}
\end{lstlisting} \label{code:caffe}

\lstset{language=Python,caption={MNIST network in PyTorch.}}
\begin{lstlisting}{language=Python}
self.fc1 = nn.Linear(28*28, 128)   # Topology
self.fc2 = nn.Linear(128, 10)
x = x.view(-1, 28, 28)             # Activations
x = F.relu(self.fc1(x))
X = self.fc2(x)
\end{lstlisting} \label{code:pytorch}

\lstset{language=Python,caption={MNIST network in Tensorflow using the Keras API.}}
\begin{lstlisting}
keras.Sequential([
  keras.layers.Flatten(input_shape=(28, 28)),
  keras.layers.Dense(128, activation=tf.nn.relu),
  keras.layers.Dense(10, activation=tf.nn.softmax)
])
\end{lstlisting} \label{code:keras}

\lstset{language=Python, caption={MNIST network in Theano using the Lasagne API.}}
\begin{lstlisting}
l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),
           input_var=input_var)
l_hid = lasagne.layers.DenseLayer(l_in, num_units=128,
           nonlinearity=lasagne.nonlinearities.rectify)
l_out = lasagne.layers.DenseLayer(
           l_hid, num_units=10,
           nonlinearity=lasagne.nonlinearities.softmax)
\end{lstlisting} \label{code:lasagne}

In terms of learning the frameworks are diverse, although gradient descent 
and auto-differentiation (where transformations are automatically registered and 
later derived using the chain rule) are among the most common features 
(seen in Tensorflow, PyTorch, CNTK, Theano and Caffe). 

The Open Neural Network Exchange Format (ONNX) is an open data format for the representation
of \gls{DL} learning models \cite{ONNX2018}. 
ONNX is interesting in this context because it, like all the frameworks above, describes 
networks as directed graphs, defined by nodes of a certain dimension (shape) connected through
edges with certain activations (operations).

\subsubsection{Neural networks in Futhark}
\index{Futhark}
The data-parallel language Futhark is designed to produce efficient parallel
code, suitable for parallel application \cite{Henriksen2017}.
Second generation \glspl{NN} are such an application, because they can be reduced
to a large number of parallel matrix multiplications.

Futhark supports hardware acceleration through the \gls{OpenCL} framework,
and can integrate with other languages, such as \gls{Python} through the
PyOpenCL interface \cite{PyOpenCL}.

\citeauthor{Minh2018} developed a library for machine learning, which 
allows the construction of densely connected layers and gradient descent
training \cite{Minh2018}.

\subsection{Third generation}
The landscape for third generation software is less homogeneous, first and
foremost because the field is still young \cite{Maass1997}.
Secondly there are two different approaches to the evaluation of \gls{SNN}s:
through simulation on general purpose hardware or specialised analogue
(neuromorphic\index{neuromorphic}) hardware \cite{Maass1997, Davison2009, Albada2018}.

For each platform---digital or otherwise---a complete programming environment
is developed from scratch, because of the degree of specialisation
\cite{Walter2015, Lin2018}.
This section covers the most important technical details of
the environments.

\subsubsection{\Gls{SNN} simulators} \label{sec:SNN-simulators}
Based on the review of \textcite{Blundell2018} this paper will discuss
the following third generation \gls{SNN} simulators: PyNN \cite{Davison2009},
NEST \cite{Gewaltig2007}, NEURON \cite{Carnevale2007},
Brian \cite{Goodman2013} and Nengo \cite{Eliasmith2015}.

PyNN is a ``simulator-independent language''
\cite{PyNN2018} that compiled to both simulated and
accelerated architectures \cite{Davison2009}.
Technically PyNN is not a simulator but acts as an interface to any third generation
backend \cite{Davison2009}.
PyNN currently interfaces with Brian, NEST, BrainScaleS and SpiNNaker
\index{Brian}\index{NEST}\index{BrainScaleS}\index{SpiNNaker}
and is more than 10 years old, \cite{Davison2009} older
than most neuromorphic chips. 
Their \gls{API}s were designed a priori and lacked a number of crucial
elements,\footnote{In particular in relation to the mapping of neuron population
onto the physical chip, see \ref{sec:Cairo}.} which the hardware designers sought
to resolve by augmenting the interface \cite{Pfeil2013, PyNN2018}.
The result is a fragmented environment that supports basic morphologies, 
but where each experiment requires retrofitting to execute correctly on the 
backends \cite{PyNN2018}.

Nengo is a neural simulation environment for large-scale neural models, with
a focus on graphical modelling \cite{Eliasmith2015}. 
The Nengo project bases itself on the Neural Engineering Framework (NEF)
\index{Neural Engineering Framework} that offers a concise language for
describing third generation simulations \cite{Bekolay2014}, along with
the limited rendering of logical computations (such as logical gates and basic
mathematical functions) into approximated
\gls{NN} structures \cite{Eliasmith2004, Eliasmith2015}.
Nengo supports a wide range a backends---non-spiking networks through Tensorflow,
simulated spiking networks through its custom \gls{OpenCL} engine and 
hardware accelerated networks through the neuromorphic platform SpiNNaker---
but have a limited repertoire of models compared to other simulators
\cite{Nengo2018}.

Listings \ref{code:Nengo} and \ref{code:PyNN} shows how
basic spiking models are defined using Nengo and PyNN.

\begin{minipage}{\linewidth}
\lstset{caption={A simple LIF MNIST population network in Nengo.}, label=code:Nengo}
\begin{lstlisting}
pop_1 = nengo.Ensemble(nengo.LIF(100), 2)
pop_2 = nengo.Ensemble(nengo.LIF(10),  1)
nengo.Connection(pop_1, pop_2)
\end{lstlisting}
\end{minipage}

\begin{minipage}{\linewidth}
\lstset{caption={A simple LIF MNIST network in PyNN.}, label=code:PyNN}
\begin{lstlisting}
pop_1 = nest.Create('iaf_exp_cond', 100)
pop_2 = nest.Create('iaf_exp_cond',  10)
nest.Connect(pop_1, pop_2, 'all_to_all')
\end{lstlisting} 
\end{minipage}
\\[0.3cm]
PyNN and Nengo both attempt to converge platform differences into one single \gls{API}, and
offer high-level description of networks with support for detailed 
configuration.
Nengo also offers an approximated model that can be evaluated in Tensorflow
\cite{Hunsberger2015}, but it does not translate to other simulators like PyNN:
models written for one simulator cannot be transferred to another \cite{Nengo2018}.

The NEST simulator supports both point neurons \index{neuron!point}
and compartmentalised models to support speed as well as sophisticated neuron
geometry \cite{Gewaltig2007}.
NEST claims to focus on ``dynamics, size and structure rather than on the detailed
morphological and biophysical properties of individual neurons''
\cite{Gewaltig2007}.
It has modelled a large number of neuron models and optimisations
\cite{Blundell2018}.

NEURON targets complex and detailed simulations of multi-chamber models, and
attempts to model all aspects of the biophysical properties \cite{Carnevale2007}.
Brian strikes somewhere between NEST and NEURON because it allows users to
inject their own models through custom equations in plain text \cite{Goodman2013}.

\textcite{Rueckauer2017} implemented a ``Spiking neural network conversion
toolbox'' that converts second generation \glspl{NN} into \glspl{SNN}.
As shown in Section \ref{sec:coding} they approach this by estimating the 
firing rate of LIF \index{LIF} neurons through fixed current inputs,
assuming even Poisson\index{Poisson distribution} distributed signals
throughout the network \cite{Rueckauer2017}.

\subsubsection{Neuromorphic hardware} \label{sec:similar-neuromorphic}
Based on the review of \textcite{Walter2015} and the work from
\textcite{Lin2018}, the following section classifies neuromorphic hardware into
two categories: either as digital interpretations of neural components, 
or as analogue emulations of neural tissue.

Digital neuromorphic chips digitises the neural signals and mimics neuron
behaviour either through the regular \gls{vonNeumann}, or
via custom digital components \cite{Walter2015}.
SpiNNaker is an example of the former, where a number of ARM \gls{ARM}
processors that are equipped with controllers for handling timers and
interrupts \cite{Walter2015}.
This permits SpiNNaker to compute arbitrary logic, while retaining
a large degree of parallelism \cite{Albada2018}.
IBM's TrueNorth \index{IBM!TrueNorth} and Intel's Loihi \index{Intel!Loihi}
are examples of neuromorphic hardware with custom digital components 
\cite{Walter2015, Lin2018}.
TrueNorth consist of 4096 independently operating neurosynaptic cores, each implementing 256
digital neurons in silicon \cite{Walter2015, ArtificialBrains2018}.
The Loihi seems similar to the TrueNorth chip, with the difference that
its 128 neuromorphic cores feature programmable
synaptic learning rules \cite{Lin2018}.

Analogue neuromorphic chips construct circuits that equals those of biological
neurons \cite{Walter2015}.
BrainScaleS \cite{Schmitt2017}, Neurogrid \cite{BrainsInSilicon2018}, 
and ROLLS (Reconfigurable On-line Learning Spiking)
\cite{Walter2015} are examples of such chips.

BrainScaleS is built on the High Input Count Analog Neural Network (HICANN)
chip, that contains up to 512 neurons depending on the hardware configuration 
\cite{Pfeil2013}.
Several HICANN chips can be integrated to allow the simulation of larger
networks, where dedicated \gls{FPGA}s set weights for each neuron and
communicate with other \gls{FPGA}s on chip \cite{Walter2015}. 
Neurogrid models around $10^6$ two-compartment neurons, where the dendritic
tree is separated from the neuron `soma' \cite{Walter2015}.
The spikes are transmitted digitally through \gls{RAM} \cite{Walter2015}.
The ROLLS processor consists of 256 analogue silicon neurons with
$\sim1.3 \cdot 10^5$ synapses, but with fixed synaptic weights
\cite{Walter2015}.

\section{DSL requirements} \label{sec:requirements}
This section sketches four functional requirements for a \gls{DSL} that 
fulfils the necessary conditions to allow the testing of the thesis hypothesis.
The requirements steers the specification as defined in section 
\ref{sec:volr} and later the implementation details in section
\ref{sec:implementation}.

\paragraph{1. Semantic consistency}
The overarching goal of the \gls{DSL} is to allow the translation 
of \gls{NN} descriptions into semantically similar models in backend runtime
environments.
In other words a network described in the \gls{DSL} should carry
the same semantic structure when translated to second or third generation
implementations. 

Because of the diverse and incompatible nature of spiking neural network
landscape, this is a non-trivial requirement,
but a necessity one if the models are to be validated across \gls{NN} paradigms.
This requirement is approached empirically, by illustrating examples in
both generations and validate whether they achieved the desired degree
of external validity.

\paragraph{2. Translation to second and third generation}
A second requirement is the translation of the \gls{DSL} into
three runtime environments to permit a sufficient degree of
generalisation.
It is required that the DSL can translate code that evaluate networks
in second generation, simulated third generation, as well as
analog third generation (neuromorphic).

\paragraph{3. Learning}
It is furthermore required that the \gls{DSL} supports a form of
learning, to illustrate the expected theoretical adaptation.
For this purpose, supervised learning through backpropagation\index{backpropagation}
is sufficient for the purpose of this thesis.

\paragraph{4. Well-typed}
As a final functional requirement the \gls{DSL} is designed to ensure
consistency and disallow any networks that are not well-formed at
compile time.
\\[0.4cm]
\noindent
None of the above mentioned environments fulfil all four requirements.
Models built in Nengo and PyNN\index{Nengo}\index{PyNN} can be evaluated
in both second and third generation environments, but Nengo does not 
offer consistent semantics between the backends and PyNN only allows
for a partial translation into the neuromorphic platforms.
However, PyNN does support a consistent \gls{API} to describe models that, at
least topologically, translates to both simulated and accelerated 
backends.

The following two sections will present the design and implementation of
Volr, while drawing on the requirements above to assess whether and
how they are met.

\section{DSL specification} \label{sec:volr}
\input{chapters/volr/volr}

\section{DSL implementation} \label{sec:implementation}
\input{chapters/volr/implementation}

\section{DSL verification} \label{sec:verification}
\input{chapters/volr/testing}

\end{document}
