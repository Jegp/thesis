\documentclass[report.tex]{subfiles}
\begin{document}

This first chapter of the thesis motivates and defines
a \gls{DSL} for the modelling of general \gls{NN}s.
The first part of the chapter presents the theory behind second and third
generation \gls{NN}s, so as to posit requirements for the language. 
Finally this chapter will present the \gls{DSL} \index{Volr},
as a means to translate cognitive concepts into computational \gls{NN}
models.

\section{Neural networks}
\Gls{NN}s is a broad term that originates in the neuronal models from
biological brain \cite{Dayan2001}.
The general architecture of neural systems can be explained as circuits
of neurons \index{neuron} connected through weighted edges.
\cite{Russel2007, Dayan2001}.
In this abstract sense a neuron is defined as a computational unit that
takes a number of signals (inputs) and processes them through some
function $f$ that outputs a single value \cite{Eliasmith2004}.
From that perspective neural networks simply \textit{computes} an 
output based on some input why neural networks can be understood as
complex non-linear computations \cite{Eliasmith2004, Dayan2001}.

In a more concrete sense neural networks computes over either
continuous (e. g. voltage and numbers) or discrete signal, and they
can be modelled with or without a temporal dimension
\cite{Eliasmith2004, Russel2007, Schmidhuber2014}.

Common for many of the models are that they enrich the
input signals ($x$) with a weight as shown in \ref{eq:weightedsum}
\cite{Schmidhuber2014, Russel2007}. 
Given $n$ input neurons, the weighted sum is the value of each
input neuron scaled by a weight for that individual neuron.
Weights allow the model to adapt the relative importance of each
input neuron by modifying their weights, thus allowing the
model to \textit{train} to a given domain \cite{Schmidhuber2014, Russel2007}.

\begin{equation} \label{eq:weightedsum}
u = w \cdot x = \sum_{i=1}^n w_i x_i
\end{equation}

Discrete models without a temporal dimension were the foundation for
the first generation of neural networks \cite{Russel2007, Maass1997}.
They are based on the perceptron model as seen in equation
\ref{eq:perceptron}, also known as the McCulluch-Pitts neuron model
\cite{Eliasmith2004}.

\begin{equation} \label{eq:perceptron}
f(x) = \begin{cases}
	 1 & \text{if } u > 0\\
	 0 & \text{otherwise}
       \end{cases}
\end{equation}

Second generation neural networks augment the perceptron model by
a) allowing continuous output values of a neuron and b) parameterising
the computation of the neuron by adding an \textit{activation function}
\index{activation function} for when the output ``activates'' 
\cite{Maass1997}.
\textit{Sigmoidal} functions are commonly used (see figure \ref{fig:sigmoid})
for activation functions
because they resemble the step function from the perceptron while 
retatining continuity \cite{Maass1997}.

\begin{figure}
\centering
\begin{tikzpicture}[domain=-6:7,xscale=0.3]
  \draw[->] (-6.2,0) -- (7.2,0) node[right] {$x$};
  \draw[->] (0,-0.2) -- (0,1.2) node[above] {$\sigma(x)$};
  \draw plot (\x,{1 / (1 + exp(-\x))}) node[right] {$\sigma(x) = {1 \over 1 + e^{-x}}$};
\end{tikzpicture}
\caption{A sigmoidal logistic function.}
\label{fig:sigmoid}
\end{figure}

A variation of this is based 

Several variations of neural models exist within neuroscience, but they

The artificial flavour is 
circuits of computational
units connected through weighted edges \cite{Dayan2001, Eliasmith2004}.

% TODO: Distributions of properties instead of actual properties

% Both types of network are architecturally similar, and both are conceived from
% the same physiological principles \autocite{dayan2001, russel2007, Nilsson2009, schmidhuber2014}.
% The implementations, however, vary greatly.

% To ensure internal and external validity in and between the two network types,
% it is desirable that the models are as closely related from a theoretical and
% practical perspective as possible.
% Additionally, to test the hypothesis, it is required that both the artificial
% and spiking models can be simulated on regular machine architecture, while
% the spiking model requires a neuromorphic hardware platform.

% An optimal approach would be to find a tool that leverages the similarities
% of the network types, while integrating with the diverse simulated or emulated
% targets.
% That is, an abstract model of neural networks that can translate into
% heterogeneous back-ends, while retaining a high degree of inter-model validity.

% A number of general frameworks for artificial neural networks
% exist\footnote{
%   Among others, see \autocite{ONNX2018}, \autocite{PyTorch2018}, \autocite{TensorFlow2018},
%   \autocite{Keras2018} and DyNet \autocite{Neubig2017}.
% }, but none of them extend to the spiking domain.
% Conversely a number of choices exist for neuromorphic modelling\footnote{
%   %TODO: Find sources on internal IBM/Intel stuff
% }, but they exclusively evaluate to neuromorphic or spiking neural network
% backends \autocite{Jordan2018}.

\subsection{Learning in neural networks}

\subsubsection{Backpropagation}

\section{DSL requirements}

\section{Similar work}

\include{chapters/volr/volr}
\end{document}
