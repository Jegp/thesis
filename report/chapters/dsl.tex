\documentclass[report.tex]{subfiles}
\begin{document}

This chapter presents the \gls{DSL} Volr \index{Volr}.
Before this presentation existing work in the simulation
of second and third generation \gls{NN}s is
analysed to provide an overview of the contemporary landscape.
Together with the theory presented above, this analysis is
exploited to define necessary requirements for the design of
the \gls{DSL}.
These requirements will frame the presentation of the \gls{DSL}, 
which ultimately aims to translate cognitive concepts into
computational \gls{NN} models.

\section{Similar work}
A vast amount of work has been put into the development of software for simulating
neural networks.
This section covers the most popular and recent projects for both second and third
generation frameworks, and extracts relevant
features for use in the requirements section \ref{sec:requirements}.

\subsection{Second generation software}
The perhaps most notable product for this type of networks is the Tensorflow \index{Tensorflow}
framework \cite{Abadi2016}.
Tensorflow is essentially an infrastructure for the description and execution of directed graph 
structures,
that connects varying activation functions and learning mechanisms through the common abstraction
of tensors \cite{Abadi2015}.
It is a large collaboration of multiple companies and organisations, who have
grown a comprehensive library of both code as well as infrastructure, and they
provide extensive hardware acceleration \cite{Abadi2015}.

The primary advantage of Tensorflow \index{Tensorflow} arrives from 
its foundation in tensors as a general abstraction that
can be applied to a wide array of problems \cite{Abadi2016}.
Other frameworks have adapted a similar approach, such as PyTorch \cite{PyTorch2018}, 
scikit-learn \cite{Sklearn2018}, Microsoft Cognitive Toolkit (CNTK) \cite{CNTK2018},
Caffe \cite{Caffe2018} and Theano \cite{Theano2018}.
The mentioned products differ slightly in terms of syntax and objective\footnote{Especially
PyTorch and Caffe targets \gls{DL}} as well as integrations for data and services, but
all rely on second generation \gls{NN} architecture.

Lasagne and Keras are examples of products that works with higher-level abstractions,
building on Theano and Tensorflow respectively \cite{Lasagne2018, Keras2018}.
They both provide imperative \gls{API}s for constructing models in steps, while
including useful utilities for the molding of data to fit the underlying tensor structures.

In terms of learning the frameworks are diverse, although gradient descent 
and auto-differentiation are among the most common features 
(seen in Tensorflow, PyTorch, CNTK, Theano
and Caffe). 

The Open Neural Network Exchange Format (ONNX) is an open data format for the representation
of \gls{DL} learning models \cite{ONNX2018}. 
In this context ONNX is interesting because it attempts to describe the networks as 
a directed graph, just like Tensorflow.
ONNX is supported by Caffe, CNTK and Pytorch, and translate to Tensorflow,
indicating that a directed graph is 
sufficiently generic to model the structure and learning tasks within 
second generation networks.

% TODO: Add example code

\subsection{Third generation software}
The landscape for third generation software is less homogeneous, first and
foremost because the field is still young \cite{Maass1991}.
Secondly there are two different approaches to constructing \gls{SNN}s:
physical or virtual \cite{Maass1997, Davison2009, Albada2018}. %TODO: Check citation

\subsubsection{\Gls{SNN} simulators}
\subsubsection{Neuromorphic hardware}
%PyNN is a simulator-independent

\subsection{Third generation modelling tools}
Before proceeding to the DSL requirements, it is worth introducing
the PyNN \index{PyNN} project \cite{Davison2009}. 
PyNN is a modelling \gls{API} for \gls{Python} that to some degree translates
third generation topologies and physiological descriptors into both simulated
and analogue backends \cite{Davison2009}.
In theory PyNN can target any third generation backend, but has chosen to focus
on projects associated with the \gls{HBP}: NEST, BrainScaleS and SpiNNaker
\index{NEST}\index{BrainScaleS}\index{SpiNNaker}
\cite{Davison2009} %TODO: Check source
It offers decent support for many of the NEST simulation features, but
requires significant amount of programming to post-fit the models to 
run on SpinNaker and BrainScales. %TODO: Add source
This is partly due to the heterogeneous architectures of the backends, but
also because projects such as PyNN faces a difficult trade-off; ease-of-use
versus specificity. %TODO: Add source
PyNN is an attempt to bridge the gap between the vastly different backends
for \gls{SNN}, similar to the Keras and Lasagne projects, but both these
projects have made significant simplifications to broaden their audiences
%TODO: Add source

%TODO: Finish this

\section{DSL requirements} \label{sec:requirements}

% Both types of network are architecturally similar, and both are conceived from
% the same physiological principles \autocite{dayan2001, russel2007, Nilsson2009, schmidhuber2014}.
% The implementations, however, vary greatly.

% To ensure internal and external validity in and between the two network types,
% it is desirable that the models are as closely related from a theoretical and
% practical perspective as possible.
% Additionally, to test the hypothesis, it is required that both the artificial
% and spiking models can be simulated on regular machine architecture, while
% the spiking model requires a neuromorphic hardware platform.

% An optimal approach would be to find a tool that leverages the similarities
% of the network types, while integrating with the diverse simulated or emulated
% targets.
% That is, an abstract model of neural networks that can translate into
% heterogeneous back-ends, while retaining a high degree of inter-model validity.

% A number of general frameworks for artificial neural networks
% exist\footnote{
%   Among others, see \autocite{ONNX2018}, \autocite{PyTorch2018}, \autocite{TensorFlow2018},
%   \autocite{Keras2018} and DyNet \autocite{Neubig2017}.
% }, but none of them extend to the spiking domain.
% Conversely a number of choices exist for neuromorphic modelling\footnote{
%   %TODO: Find sources on internal IBM/Intel stuff
% }, but they exclusively evaluate to neuromorphic or spiking neural network
% backends \autocite{Jordan2018}.

\include{chapters/volr/volr}
\end{document}
