\documentclass[report.tex]{subfiles}
\begin{document}

This first chapter of the thesis motivates and defines
a \gls{DSL} for the modelling of general \gls{NN}s.
The first part of the chapter presents the theory behind second and third
generation \gls{NN}s, so as to posit requirements for the language. 
Finally this chapter will present the \gls{DSL} \index{Volr},
as a means to translate cognitive concepts into computational \gls{NN}
models.

\section{Neural networks}
\Gls{NN}s is a broad term that originates in the neuronal models from
biological brain \cite{Dayan2001}.
The general architecture of neural systems can be explained as circuits
of neurons \index{neuron} connected through weighted edges.
\cite{Russel2007, Dayan2001}.
In this abstract sense a neuron is defined as a computational unit that
takes a number of signals (inputs) and processes them through some
function $f$ that outputs a single value \cite{Eliasmith2004}.
From that perspective neural networks simply \textit{computes} an 
output based on some input why neural networks can be understood as
complex non-linear computations \cite{Eliasmith2004, Dayan2001}.

In a more concrete sense neural networks computes over either
continuous (e. g. voltage or numbers) or discrete signal, and they
can be modelled with or without a temporal dimension
\cite{Eliasmith2004, Nilsson2009}.

Discrete models without a temporal dimension were the foundation for
the first perceptron models, as seen in equation \ref{eq:perceptron}.

\begin{figure}
  
\end{figure}

\gls{ANN} this design has been 
Several variations of neural models exist within neuroscience, but they

The artificial flavour is 
circuits of computational
units connected through weighted edges \cite{Dayan2001, Eliasmith2004}.

% TODO: Distributions of properties instead of actual properties

% Both types of network are architecturally similar, and both are conceived from
% the same physiological principles \autocite{dayan2001, russel2007, Nilsson2009, schmidhuber2014}.
% The implementations, however, vary greatly.

% To ensure internal and external validity in and between the two network types,
% it is desirable that the models are as closely related from a theoretical and
% practical perspective as possible.
% Additionally, to test the hypothesis, it is required that both the artificial
% and spiking models can be simulated on regular machine architecture, while
% the spiking model requires a neuromorphic hardware platform.

% An optimal approach would be to find a tool that leverages the similarities
% of the network types, while integrating with the diverse simulated or emulated
% targets.
% That is, an abstract model of neural networks that can translate into
% heterogeneous back-ends, while retaining a high degree of inter-model validity.

% A number of general frameworks for artificial neural networks
% exist\footnote{
%   Among others, see \autocite{ONNX2018}, \autocite{PyTorch2018}, \autocite{TensorFlow2018},
%   \autocite{Keras2018} and DyNet \autocite{Neubig2017}.
% }, but none of them extend to the spiking domain.
% Conversely a number of choices exist for neuromorphic modelling\footnote{
%   %TODO: Find sources on internal IBM/Intel stuff
% }, but they exclusively evaluate to neuromorphic or spiking neural network
% backends \autocite{Jordan2018}.

\subsection{Learning in neural networks}

\subsubsection{Backpropagation}

\section{DSL requirements}

\section{Similar work}

\include{chapters/volr/volr}
\end{document}
