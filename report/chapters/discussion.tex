\documentclass[report.tex]{subfiles}
\begin{document}


% TODO: Functional requirements

This thesis sat out to explore \glspl{SNN} and their future relevance to the field of
\gls{ml}.
It did so by putting forward two hypotheses and by designing experiments to
falsify the hypotheses.
During the thesis, a \gls{DSL} for neural models was presented, along with two \gls{ml}
supporting \gls{ml} libraries for the training of second and third generation
\glspl{NN}. 
For the latter, a theory for the translation of model parameters was developed
and emperically tested.

Three experiments, each executed on two different backends, were conducted to
prove two things: that the \gls{DSL} VOLR can translate into second and third
generation neural networks and adapt to a well-known recognition task by
backpropagation learning.

The experimental results prove that the \gls{DSL} concepts indeed are
translatable between the \gls{NN} paradigms, and that the \gls{DSL} can generate
executable programs that retain the abstract network topologies.

The results further showed that some form of learning was taking place in the
spiking experiments
However, flaws in the gradient approximation model and the spike rate coding scheme
caused the model to learn consistently wrong patterns and produce a large
quantity of dead neurons.
The experiments does not disprove that training within \gls{SNN} is possible,
but further adaptations to the gradient and coding models are required.
\\[0.1cm]

Table \ref{tab:findings} concludes the findings in the thesis:

\def\arraystretch{1.5}
\begin{table}
  \centering
  \begin{tabular}{l c c}
     Hypothesis 1 & Translation to \gls{ANN} & Confirmed \\ \hline
     Hypothesis 1 & Translation to \gls{SNN} & Confirmed \\ \hline
     Hypothesis 2 & Learning an MNIST task in Futhark & Confirmed \\ \hline
     Hypothesis 2 & Learning an MNIST task in NEST & Unconfirmed \\ \hline
  \end{tabular}
  \caption{A summary of the thesis findings.}
  \label{tab:findings}
\end{table}

While this thesis focused on the theoretical foundations of the DSL, its hypotheses were conceived with the assumption that the
technical tools for constructing and simulating spiking neural networks were in place.
During the experimental phase of the project, this assumption proved to be wrong.
For future research, an early benchmark of framework candidates for DSL backends should be part of early initial feasibility studies to avoid similar obstacles. For this reason hypothesis 2 could not conclusively be confirmed or disproved. The corresponding experiment could not be conducted.

Research within \gls{ml}---surrounding \glspl{NN} in particular---offers a large
number of optimisation techniques, which could be employed to improve the above
models.
It is common to operate with a momentum in the learning rate, such that
larger errors cause larger adaptations \cite{Montavon1998, Sutskever2013}.
It is also popular to add a layer normalisation scheme to force the layers to
adhere to a certain property such as a sigmoid distribution.
This could be attempted to avoid the large number of dead neurons, because the
layers could be normalised into a distribution that minimises the likelihood of
zero signals.

In the context of optimisation, the spike rate models are another obvious point
of improvement.
The current rate models ignore the amplitude and inter-spike intervals, and are
effectively discarding valuable informatino.
A possible next step could be to explore other coding schemes that
allows for more stable and concise gradient models.

To improve training and learning of the models, alternative paradigms can be explored that do not
rely on differentiability. 
One example is evolution learning, where the search for optima is more time 
consuming compared to gradient descent, but that can be done without requiring a gradient model.

Another step to avoid dead neurons is to explore different activation functions. 
The currently used ReLU function\index{ReLU} as flawed in two ways: it is not
differentiable around zero and it does not penalise values close to
zero.
Other activation functions, such as the sigmoid function---or a linear term
that favours larger weights---could deincentivise dead neurons.
\\[0.1cm]

The thesis also touched on the subject of cognitive science, because the field
is highly invested in the prospects of the simulation of neurophysiology.
Regarding the cognitive REF theory in the context of this thesis, however, not much can be concluded.
It can be said though, that it is necessary to analyse far bigger systems before
any connections can be made.
The work done in this thesis to compose neural components are an essential first
step towards being able to not just construct, but also understand, the neural
network models and its semantics.
\\[0.1cm]

The PyNN interface has proven to be less stable and scaleable than anticipated.
However, the architecture of the \gls{DSL} permits it to generalise to numerous
other backends.
The Haskell compiler is entirely independent from the experiment
results and, since the because the underlying neural network model stays constant, 
it is a small matter to compile to other targets such as BrainScaleS or
SpiNNaker.
This could also be used to omit the unstable PyNN interface and compile direclty
to NEST or a similar framework.

As a final perspective one can imagine the advantages of models that seamlessly
compile between platforms. 
Particularly in the context of neuromorphic computing.
If a better model for the parameter translation is found, \gls{ANN} models can
be compiled directly to neuromorphic hardware.
One application is recognition models for faces or objects, but if it is
possible to translate functionally complete logical gates (like the NAND gate)
into neural circuits, any logical circuit can be translated.
This extends to algorithmic problems like sorting.
If the accuracies can be improved and translated into neuromorphic hardware, it
would accelerate the current computational capacities of the von Neumann machine
architectures by a factor of at least 100.
\\[0.1cm]

\FloatBarrier

\end{document}
